{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c02861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:32.554174Z",
     "start_time": "2023-07-22T10:31:29.299974Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD, FastICA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, RobustScaler\n",
    "from scipy.stats import ttest_rel\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from scripts import tabular_metrics\n",
    "from scripts.differentiable_pfn_evaluation import eval_model, eval_model_range\n",
    "from scripts.model_builder import (\n",
    "    get_model,\n",
    "    get_default_spec,\n",
    "    save_model,\n",
    "    load_model,\n",
    "    load_model_only_inference,\n",
    ")\n",
    "from scripts.transformer_prediction_interface import (\n",
    "    transformer_predict,\n",
    "    get_params_from_config,\n",
    "    TabPFNClassifier,\n",
    ")\n",
    "\n",
    "from datasets import (\n",
    "    load_openml_list,\n",
    "    open_cc_dids,\n",
    "    open_cc_valid_dids,\n",
    "    test_dids_classification,\n",
    ")\n",
    "\n",
    "base_path = '.'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set the seed for the random number generator provided by PyTorch\n",
    "torch.manual_seed(0)\n",
    "# Set the seed for the random number generator provided by NumPy\n",
    "np.random.seed(0)\n",
    "# Set the seed for the random number generator provided by Python\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6df194",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cb029b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:38.271068Z",
     "start_time": "2023-07-22T10:31:38.260847Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_datasets(selector, task_type, suite='cc'):\n",
    "    if task_type == 'binary':\n",
    "        ds = valid_datasets_binary if selector == 'valid' else test_datasets_binary\n",
    "    else:\n",
    "        if suite == 'openml':\n",
    "            ds = valid_datasets_multiclass if selector == 'valid' else test_datasets_multiclass\n",
    "        elif suite == 'cc':\n",
    "            ds = cc_valid_datasets_multiclass if selector == 'valid' else cc_test_datasets_multiclass\n",
    "        else:\n",
    "            raise Exception(\"Unknown suite\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d538e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:38.736541Z",
     "start_time": "2023-07-22T10:31:38.717096Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_for_evaluation(dataset_index, test_datasets):\n",
    "    dataset = test_datasets[dataset_index]\n",
    "    dataset_name = dataset[0]\n",
    "    dataset_size = dataset[1].shape\n",
    "\n",
    "    print(f'Evaluation dataset name: {dataset_name} size {dataset_size} --- {dataset_index}/{len(test_datasets)}')\n",
    "\n",
    "    xs, ys = dataset[1].clone(), dataset[2].clone()\n",
    "    # ------------------------------------------------\n",
    "    # Replace NaN values with 0\n",
    "    xs[torch.isnan(xs)] = 0 #todo \n",
    "    ys[torch.isnan(ys)] = 0\n",
    "    # ------------------------------------------------\n",
    "    eval_position = xs.shape[0] // 2\n",
    "    train_xs, train_ys = xs[0:eval_position], ys[0:eval_position]\n",
    "    test_xs, test_ys = xs[eval_position:], ys[eval_position:]\n",
    "    return train_xs, train_ys, test_xs, test_ys, dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6a05251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.436676Z",
     "start_time": "2023-07-22T10:31:39.788499Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 30\n",
      "Loading balance-scale 11 ..\n",
      "Loading mfeat-fourier 14 ..\n",
      "Loading breast-w 15 ..\n",
      "Loading mfeat-karhunen 16 ..\n",
      "Loading mfeat-morphological 18 ..\n",
      "Loading mfeat-zernike 22 ..\n",
      "Loading cmc 23 ..\n",
      "Loading credit-approval 29 ..\n",
      "Loading credit-g 31 ..\n",
      "Loading diabetes 37 ..\n",
      "Loading tic-tac-toe 50 ..\n",
      "Loading vehicle 54 ..\n",
      "Loading eucalyptus 188 ..\n",
      "Loading analcatdata_authorship 458 ..\n",
      "Loading analcatdata_dmft 469 ..\n",
      "Loading pc4 1049 ..\n",
      "Loading pc3 1050 ..\n",
      "Loading kc2 1063 ..\n",
      "Loading pc1 1068 ..\n",
      "Loading banknote-authentication 1462 ..\n",
      "Loading blood-transfusion-service-center 1464 ..\n",
      "Loading ilpd 1480 ..\n",
      "Loading qsar-biodeg 1494 ..\n",
      "Loading wdbc 1510 ..\n",
      "Loading cylinder-bands 6332 ..\n",
      "Loading dresses-sales 23381 ..\n",
      "Loading MiceProtein 40966 ..\n",
      "Loading car 40975 ..\n",
      "Loading steel-plates-fault 40982 ..\n",
      "Loading climate-model-simulation-crashes 40994 ..\n",
      "Number of datasets: 150\n",
      "Loading breast-cancer 13 ..\n",
      "Loading colic 25 ..\n",
      "Loading dermatology 35 ..\n",
      "Loading sonar 40 ..\n",
      "Loading glass 41 ..\n",
      "Loading haberman 43 ..\n",
      "Loading tae 48 ..\n",
      "Loading heart-c 49 ..\n",
      "Loading heart-h 51 ..\n",
      "Loading heart-statlog 53 ..\n",
      "Loading hepatitis 55 ..\n",
      "Loading vote 56 ..\n",
      "Loading ionosphere 59 ..\n",
      "Loading iris 61 ..\n",
      "Loading wine 187 ..\n",
      "Loading flags 285 ..\n",
      "Loading hayes-roth 329 ..\n",
      "Loading monks-problems-1 333 ..\n",
      "Loading monks-problems-2 334 ..\n",
      "Loading monks-problems-3 335 ..\n",
      "Loading SPECT 336 ..\n",
      "Loading SPECTF 337 ..\n",
      "Loading grub-damage 338 ..\n",
      "Loading synthetic_control 377 ..\n",
      "Loading prnn_crabs 446 ..\n",
      "Loading analcatdata_lawsuit 450 ..\n",
      "Loading irish 451 ..\n",
      "Loading analcatdata_broadwaymult 452 ..\n",
      "Loading analcatdata_reviewer 460 ..\n",
      "Loading backache 463 ..\n",
      "Loading prnn_synth 464 ..\n",
      "Loading schizo 466 ..\n",
      "Loading profb 470 ..\n",
      "Loading analcatdata_germangss 475 ..\n",
      "Loading biomed 481 ..\n",
      "Loading rmftsa_sleepdata 679 ..\n",
      "Loading diggle_table_a2 694 ..\n",
      "Loading rmftsa_ladata 717 ..\n",
      "Loading pwLinear 721 ..\n",
      "Loading analcatdata_vineyard 724 ..\n",
      "Loading machine_cpu 733 ..\n",
      "Loading pharynx 738 ..\n",
      "Loading auto_price 745 ..\n",
      "Loading servo 747 ..\n",
      "Loading analcatdata_wildcat 748 ..\n",
      "Loading pm10 750 ..\n",
      "Loading wisconsin 753 ..\n",
      "Loading autoPrice 756 ..\n",
      "Loading meta 757 ..\n",
      "Loading analcatdata_apnea3 764 ..\n",
      "Loading analcatdata_apnea2 765 ..\n",
      "Loading analcatdata_apnea1 767 ..\n",
      "Loading disclosure_x_bias 774 ..\n",
      "Loading bodyfat 778 ..\n",
      "Loading cleveland 786 ..\n",
      "Loading triazines 788 ..\n",
      "Loading disclosure_x_tampered 795 ..\n",
      "Loading cpu 796 ..\n",
      "Loading cholesterol 798 ..\n",
      "Loading chscase_funds 801 ..\n",
      "Loading pbcseq 802 ..\n",
      "Loading pbc 810 ..\n",
      "Loading rmftsa_ctoarrivals 811 ..\n",
      "Loading chscase_vine2 814 ..\n",
      "Loading chatfield_4 820 ..\n",
      "Loading boston_corrected 825 ..\n",
      "Loading sensory 826 ..\n",
      "Loading disclosure_x_noise 827 ..\n",
      "Loading autoMpg 831 ..\n",
      "Loading kdd_el_nino-small 839 ..\n",
      "Loading autoHorse 840 ..\n",
      "Loading stock 841 ..\n",
      "Loading breastTumor 844 ..\n",
      "Loading analcatdata_gsssexsurvey 852 ..\n",
      "Loading boston 853 ..\n",
      "Loading fishcatch 854 ..\n",
      "Loading vinnie 860 ..\n",
      "Loading mu284 880 ..\n",
      "Loading no2 886 ..\n",
      "Loading chscase_geyser1 895 ..\n",
      "Loading chscase_census6 900 ..\n",
      "Loading chscase_census5 906 ..\n",
      "Loading chscase_census4 907 ..\n",
      "Loading chscase_census3 908 ..\n",
      "Loading chscase_census2 909 ..\n",
      "Loading plasma_retinol 915 ..\n",
      "Loading visualizing_galaxy 925 ..\n",
      "Loading colleges_usnews 930 ..\n",
      "Loading disclosure_z 931 ..\n",
      "Loading socmob 934 ..\n",
      "Loading chscase_whale 939 ..\n",
      "Loading water-treatment 940 ..\n",
      "Loading lowbwt 941 ..\n",
      "Loading arsenic-female-bladder 949 ..\n",
      "Loading analcatdata_halloffame 966 ..\n",
      "Loading analcatdata_birthday 968 ..\n",
      "Loading analcatdata_draft 984 ..\n",
      "Loading collins 987 ..\n",
      "Loading prnn_fglass 996 ..\n",
      "Loading jEdit_4.2_4.3 1048 ..\n",
      "Loading mc2 1054 ..\n",
      "Loading mw1 1071 ..\n",
      "Loading jEdit_4.0_4.2 1073 ..\n",
      "Loading PopularKids 1100 ..\n",
      "Loading teachingAssistant 1115 ..\n",
      "Loading lungcancer_GSE31210 1412 ..\n",
      "Loading MegaWatt1 1442 ..\n",
      "Loading PizzaCutter1 1443 ..\n",
      "Loading PizzaCutter3 1444 ..\n",
      "Loading CostaMadre1 1446 ..\n",
      "Loading CastMetal1 1447 ..\n",
      "Loading KnuggetChase3 1448 ..\n",
      "Loading PieChart1 1451 ..\n",
      "Loading PieChart3 1453 ..\n",
      "Loading parkinsons 1488 ..\n",
      "Loading planning-relax 1490 ..\n",
      "Loading qualitative-bankruptcy 1495 ..\n",
      "Loading sa-heart 1498 ..\n",
      "Loading seeds 1499 ..\n",
      "Loading thoracic-surgery 1506 ..\n",
      "Loading user-knowledge 1508 ..\n",
      "Loading wholesale-customers 1511 ..\n",
      "Loading heart-long-beach 1512 ..\n",
      "Loading robot-failures-lp5 1520 ..\n",
      "Loading vertebra-column 1523 ..\n",
      "Loading Smartphone-Based_Recognition_of_Human_Activities 4153 ..\n",
      "Loading breast-cancer-dropped-missing-attributes-values 23499 ..\n",
      "Loading LED-display-domain-7digit 40496 ..\n",
      "Loading GAMETES_Epistasis_2-Way_20atts_0.1H_EDM-1_1 40646 ..\n",
      "Loading calendarDOW 40663 ..\n",
      "Loading corral 40669 ..\n",
      "Loading mofn-3-7-10 40680 ..\n",
      "Loading thyroid-new 40682 ..\n",
      "Loading solar-flare 40686 ..\n",
      "Loading threeOf9 40690 ..\n",
      "Loading xd6 40693 ..\n",
      "Loading tokyo1 40705 ..\n",
      "Loading parity5_plus_5 40706 ..\n",
      "Loading cleve 40710 ..\n",
      "Loading cleveland-nominal 40711 ..\n",
      "Loading Australian 40981 ..\n",
      "Loading DiabeticMellitus 41430 ..\n",
      "Loading conference_attendance 41538 ..\n",
      "Loading CPMP-2015-runtime-classification 41919 ..\n",
      "Loading TuningSVMs 41976 ..\n",
      "Loading regime_alimentaire 42172 ..\n",
      "Loading iris-example 42261 ..\n",
      "Loading Touch2 42544 ..\n",
      "Loading penguins 42585 ..\n",
      "Loading titanic 42638 ..\n"
     ]
    }
   ],
   "source": [
    "max_samples = 10000\n",
    "bptt = 10000\n",
    "\n",
    "cc_test_datasets_multiclass, cc_test_datasets_multiclass_df = load_openml_list(open_cc_dids, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = max_samples, num_feats=100, return_capped=True)\n",
    "cc_valid_datasets_multiclass, cc_valid_datasets_multiclass_df = load_openml_list(open_cc_valid_dids, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = max_samples, num_feats=100, return_capped=True)\n",
    "\n",
    "# Loading longer OpenML Datasets for generalization experiments (optional)\n",
    "# test_datasets_multiclass, test_datasets_multiclass_df = load_openml_list(test_dids_classification, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = 10000, num_feats=100, return_capped=True)\n",
    "\n",
    "random.shuffle(cc_valid_datasets_multiclass)\n",
    "\n",
    "\n",
    "model_string, longer, task_type = '', 1, 'multiclass'\n",
    "eval_positions = [1000]\n",
    "bptt = 2000\n",
    "    \n",
    "test_datasets, valid_datasets = get_datasets('test', task_type, suite='cc'), get_datasets('valid', task_type, suite='cc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01759e3",
   "metadata": {},
   "source": [
    "## Plotting Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04dd6ec",
   "metadata": {},
   "source": [
    "### Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a86427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.467785Z",
     "start_time": "2023-07-22T10:31:52.442280Z"
    }
   },
   "outputs": [],
   "source": [
    "def copy_without_keys(original_dict, keys_to_exclude):\n",
    "    return {k: v for k, v in original_dict.items() if k not in keys_to_exclude}\n",
    "\n",
    "def deepcopy_excluding_keys(original_dict, keys_to_exclude):\n",
    "    new_dict = copy_without_keys(original_dict, keys_to_exclude)\n",
    "    return new_dict\n",
    "\n",
    "def save_dict_to_pickle_file(dictionary, filename, keys_to_exclude):\n",
    "    dict_to_plot = deepcopy_excluding_keys(dictionary, keys_to_exclude) \n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(dict_to_plot, file)\n",
    "\n",
    "def clean_result_dictionary_for_evaluation(dictionary, keys_to_exclude):\n",
    "    def clean_result_dict(d, keys):\n",
    "        new_d = {}\n",
    "        for key, value in d.items():\n",
    "            if key not in keys:\n",
    "                if isinstance(value, dict):\n",
    "                    new_d[key] = copy_without_keys(value, keys)\n",
    "                else:\n",
    "                    new_d[key] = copy.deepcopy(value)\n",
    "        return new_d\n",
    "\n",
    "    new_dict = clean_result_dict(dictionary, keys_to_exclude)\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ed4d5",
   "metadata": {},
   "source": [
    "### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c794876a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.483882Z",
     "start_time": "2023-07-22T10:31:52.471300Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, result_path):\n",
    "    # Create lists to store data for the heatmap\n",
    "    datasets = []\n",
    "    ensemble_names = []\n",
    "    auc_scores = []\n",
    "    # Extract data from the ensemble_results dictionary\n",
    "    for ensemble_name, ensemble_data in ensemble_results.items():\n",
    "    \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            datasets.append(dataset_name)\n",
    "            ensemble_names.append(ensemble_name)\n",
    "            auc_scores.append(ensemble_dict['auc'])\n",
    "\n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        'Ensemble Configuration': ensemble_names,\n",
    "        'Dataset': datasets,\n",
    "        'AUC Score': auc_scores\n",
    "    })\n",
    "\n",
    "    # Reshape the DataFrame to have \"Dataset\" as rows, \"Ensemble Configuration\" as columns, and \"AUC Score\" as values\n",
    " \n",
    "    df_pivot = df.pivot(index='Ensemble Configuration', columns='Dataset', values='AUC Score')\n",
    "    # Create the heatmap using seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns_plot = sns.heatmap(df_pivot, annot=True, cmap='YlGnBu', fmt=\".2f\")\n",
    "    plt.title(\"AUC Scores of Ensemble Configurations Across All Datasets\")\n",
    "    plt.xlabel(\"Ensemble Configuration\")\n",
    "    plt.ylabel(\"Dataset\")\n",
    "    \n",
    "    plt.setp(sns_plot.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d92b31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.514967Z",
     "start_time": "2023-07-22T10:31:52.489562Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_average_weights_data_transformations(dictionary, transformation_names, result_path):\n",
    "    transformation_weight_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "       \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            if \"data_ens_dict\" not in ensemble_dict:\n",
    "                continue\n",
    "                \n",
    "            for model_transformation_key, model_transformation in ensemble_dict[\"data_ens_dict\"].items():\n",
    "            \n",
    "                transformation_name = model_transformation_key.split(\"-\")[-1]\n",
    "                \n",
    "                if transformation_name not in transformation_weight_dict:\n",
    "                    transformation_weight_dict[transformation_name] = []\n",
    "                    \n",
    "                transformation_weight_dict[transformation_name].append(model_transformation[\"data_weight\"])\n",
    "                \n",
    "    for transformation_name, data_weight_list in transformation_weight_dict.items():\n",
    "        transformation_weight_dict[transformation_name] = sum(transformation_weight_dict[transformation_name]) / len(transformation_weight_dict[transformation_name])\n",
    "\n",
    "    # Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    transformation_names = list(transformation_weight_dict.keys())\n",
    "    average_weights = list(transformation_weight_dict.values())\n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns_plot = sns.barplot(x=transformation_names, y=average_weights)\n",
    "    plt.xlabel(\"Transformations\")\n",
    "    plt.ylabel(\"Average Weight\")\n",
    "    plt.title(\"Average Weight of Transformations Across Ensembles and All Datasets\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "    # Annotate each bar with its corresponding value\n",
    "    for index, value in enumerate(average_weights):\n",
    "        plt.text(index, value, f\"{value:.2f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26549aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.546123Z",
     "start_time": "2023-07-22T10:31:52.518805Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_scatter_train_auc_test_auc_best_performer_transformations(dictionary, result_path):\n",
    "    transformation_best_performer_aucs = {\"test_auc\": [],\n",
    "                                          \"train_auc\": [],\n",
    "                                          \"split_type\":[]}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if \"best_performer\" not in ensemble_name: \n",
    "            continue\n",
    "        split_type = ensemble_name.split(\"/\")[-1]\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            test_auc = ensemble_dict[\"auc\"]\n",
    "            train_auc = -1\n",
    "            \n",
    "            for model_transformation_key, model_transformation in ensemble_dict[\"data_ens_dict\"].items():\n",
    "                if model_transformation[\"data_weight\"] == 1:\n",
    "                    train_auc = model_transformation[\"data_auc\"]\n",
    "                    break\n",
    "                    \n",
    "            transformation_best_performer_aucs[\"test_auc\"].append(test_auc)\n",
    "            transformation_best_performer_aucs[\"train_auc\"].append(train_auc)\n",
    "            transformation_best_performer_aucs[\"split_type\"].append(split_type)\n",
    "              \n",
    "    data = pd.DataFrame(transformation_best_performer_aucs)\n",
    "\n",
    "    # Create the scatterplot with hue\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns_plot = sns.scatterplot(x=\"train_auc\", y=\"test_auc\", hue=\"split_type\", data=data)\n",
    "\n",
    "    # Fit the linear regression line\n",
    "    sns_plot = sns.regplot(x=\"train_auc\", y=\"test_auc\", scatter=False, color=\"gray\", data=data)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel(\"Train AUC\")\n",
    "    plt.ylabel(\"Test AUC\")\n",
    "    plt.title(\"Predicted AUC based on train data compared to test AUC on test data of best performer transformation\")\n",
    "\n",
    "    plt.legend(title=\"Type of Train Split\")\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight', dpi=500)\n",
    "    plt.show()\n",
    "    \n",
    "    ## ------------------------------ C O R R E L A T I O N ------------------------------\n",
    "    correlation = data['test_auc'].corr(data['train_auc'])\n",
    "    print(\"Best performer: Correlation between test_auc and train_auc:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1a08fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.578175Z",
     "start_time": "2023-07-22T10:31:52.549215Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_scatter_train_auc_test_auc_weighted_average_transformations(dictionary, result_path):\n",
    "    transformation_best_performer_aucs = {\"test_auc\": [],\n",
    "                                          \"train_auc\": [],\n",
    "                                          \"split_type\":[]}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if \"weighted_average\" not in ensemble_name: \n",
    "            continue\n",
    "        split_type = ensemble_name.split(\"/\")[-1]\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            test_auc = ensemble_dict[\"auc\"]\n",
    "            train_auc = 0\n",
    "            \n",
    "            if \"data_ens_dict\" not in ensemble_dict: \n",
    "                continue\n",
    "            for model_transformation_key, model_transformation in ensemble_dict[\"data_ens_dict\"].items():\n",
    "                train_auc += model_transformation[\"data_weight\"] * model_transformation[\"data_auc\"]\n",
    "                    \n",
    "            transformation_best_performer_aucs[\"test_auc\"].append(test_auc)\n",
    "            transformation_best_performer_aucs[\"train_auc\"].append(train_auc)\n",
    "            transformation_best_performer_aucs[\"split_type\"].append(split_type)\n",
    "              \n",
    "    data = pd.DataFrame(transformation_best_performer_aucs)\n",
    "\n",
    "    # Create the scatterplot with hue\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns_plot = sns.scatterplot(x=\"train_auc\", y=\"test_auc\", hue=\"split_type\", data=data)\n",
    "\n",
    "    # Fit the linear regression line\n",
    "    sns_plot = sns.regplot(x=\"train_auc\", y=\"test_auc\", scatter=False, color=\"gray\", data=data)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel(\"Train AUC\")\n",
    "    plt.ylabel(\"Test AUC\")\n",
    "    plt.title(\"Predicted AUC based on train data compared to test AUC on test data of weighted average transformation\")\n",
    "\n",
    "    plt.legend(title=\"Type of Train Split\")\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight', dpi=500)\n",
    "    plt.show()\n",
    "    \n",
    "    ## ------------------------------ C O R R E L A T I O N ------------------------------\n",
    "    correlation = data['test_auc'].corr(data['train_auc'])\n",
    "    print(\"Weighted average: Correlation between test_auc and train_auc:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506727df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.609440Z",
     "start_time": "2023-07-22T10:31:52.582172Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_average_weights_data_transformations(dictionary, transformation_names, result_path):\n",
    "    transformation_weight_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "       \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            if \"data_ens_dict\" not in ensemble_dict:\n",
    "                continue\n",
    "                \n",
    "            for model_transformation_key, model_transformation in ensemble_dict[\"data_ens_dict\"].items():\n",
    "            \n",
    "                transformation_name = model_transformation_key.split(\"-\")[-1]\n",
    "                \n",
    "                if transformation_name not in transformation_weight_dict:\n",
    "                    transformation_weight_dict[transformation_name] = []\n",
    "                    \n",
    "                transformation_weight_dict[transformation_name].append(model_transformation[\"data_weight\"])\n",
    "                \n",
    "    for transformation_name, data_weight_list in transformation_weight_dict.items():\n",
    "        transformation_weight_dict[transformation_name] = sum(transformation_weight_dict[transformation_name]) / len(transformation_weight_dict[transformation_name])\n",
    "\n",
    "    # Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    transformation_names = list(transformation_weight_dict.keys())\n",
    "    average_weights = list(transformation_weight_dict.values())\n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns_plot = sns.barplot(x=transformation_names, y=average_weights)\n",
    "    plt.xlabel(\"Transformations\")\n",
    "    plt.ylabel(\"Average Weight\")\n",
    "    plt.title(\"Average Weight of Transformations Across Ensembles and All Datasets\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "    # Annotate each bar with its corresponding value\n",
    "    for index, value in enumerate(average_weights):\n",
    "        plt.text(index, value, f\"{value:.2f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6efd6e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.641305Z",
     "start_time": "2023-07-22T10:31:52.613098Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_average_train_auc_model(dictionary, result_path):\n",
    "    model_auc_dict = {}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if \"baseline\" == ensemble_name: \n",
    "            continue\n",
    "        \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            for model_name, model_content in ensemble_dict[\"model_dict\"].items():\n",
    "                \n",
    "                model_auc = model_content[\"model_auc\"]\n",
    "                if model_name not in model_auc_dict:\n",
    "                    model_auc_dict[model_name] = []\n",
    "                \n",
    "                model_auc_dict[model_name].append(model_auc)\n",
    "                \n",
    "    # for model_name, model_auc_list in model_auc_dict.items():\n",
    "    #     model_auc_dict[model_name] = sum(model_auc_list) / len(model_auc_list)\n",
    "                \n",
    "    # Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    \n",
    "    data = pd.DataFrame(model_auc_dict)\n",
    "    #print(data)\n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Step 1: Melt the DataFrame to transform it into a long-form\n",
    "    melted_data = data.melt()\n",
    "    # Step 2: Calculate the mean and 95% confidence intervals for each column\n",
    "    mean_data = melted_data.groupby('variable')['value'].mean().reset_index()\n",
    "    ci_data = melted_data.groupby('variable')['value'].agg(lambda x: x.sem() * 1.96).reset_index()\n",
    "    mean_data['ci'] = ci_data['value']\n",
    "\n",
    "    sns_plot = sns.barplot(x='variable', y='value', data=mean_data)\n",
    "\n",
    "    # Step 4: Add error bars with 95% confidence intervals\n",
    "    plt.errorbar(x=mean_data['variable'], y=mean_data['value'], yerr=mean_data['ci'], fmt='none', color='black', capsize=4)\n",
    "\n",
    "    # Step 5: Annotate each bar with its value\n",
    "    for index, row in mean_data.iterrows():\n",
    "        plt.text(index, row['value'] + row['ci'], f\"{row['value']:.4f}\", ha='center', va='bottom')\n",
    "\n",
    "    \n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Average AUC\")\n",
    "    plt.title(\"Average AUC of Models Across Ensembles and All Datasets\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1024a859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.672581Z",
     "start_time": "2023-07-22T10:31:52.644893Z"
    }
   },
   "outputs": [],
   "source": [
    "# ensemble_results[\"best_performer/bagging\"] = {\"dataset_name1\":{\n",
    "# \"auc\": 0.9,\n",
    "# \"model_dict\": {\n",
    "#     'gp_model_1': {\n",
    "#         'model': nn.module\n",
    "#         'model_auc' :\n",
    "#         'model_weight' :\n",
    "#     }\n",
    "#     'scm_model_1':{\n",
    "#         'model': nn.module\n",
    "#         'model_auc' :\n",
    "#         'model_weight' :\n",
    "#     }\n",
    "#     'bag_model_1':\n",
    "#     }\n",
    "# \"data_ens_dict\": {\n",
    "#         'gp_model_1_identity': {\n",
    "#             'train_data': [x , y]\n",
    "#             'test_data' : x_test\n",
    "#             'test_preds' : preds\n",
    "#             'data_auc' :\n",
    "#             'data_weight' :\n",
    "#         }\n",
    "#         'gp_model_1_powertransformer':{\n",
    "#             'train_data': [x_t , y_t]\n",
    "#             'test_data' : x_test_t\n",
    "#             'data_auc' :\n",
    "#             'data_weight' :\n",
    "#         }\n",
    "#         'bag_model_1':\n",
    "#     }             \n",
    "# },\n",
    "# ....}\n",
    "# \n",
    "\n",
    "def plot_histogram_average_test_AUC_instance(dictionary, result_path):\n",
    "\n",
    "    aucs_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        \n",
    "        if ensemble_name not in aucs_dict:\n",
    "            aucs_dict[ensemble_name] = []\n",
    "                \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            \n",
    "            aucs_dict[ensemble_name].append(ensemble_dict[\"auc\"])\n",
    "                \n",
    "                \n",
    "\n",
    "    # Calculate the averages and 95% confidence intervals for each key\n",
    "    averages = {key: sum(values) / len(values) for key, values in aucs_dict.items()}\n",
    "    conf_ints = {key: 1.96 * (pd.Series(values).std() / len(values) ** 0.5) for key, values in aucs_dict.items()}\n",
    "\n",
    "    # Convert the averages to a DataFrame\n",
    "    df = pd.DataFrame(list(averages.items()), columns=['Model', 'Average AUC'])\n",
    "    df['Confidence Interval'] = df['Model'].map(conf_ints)\n",
    "\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Create the seaborn bar plot with error bars\n",
    "    sns_plot = sns.barplot(x='Model', y='Average AUC', data=df)\n",
    "\n",
    "    # Annotate each bar with its value and the confidence interval\n",
    "    for index, row in df.iterrows():\n",
    "        sns_plot.text(index, row['Average AUC'] + row['Confidence Interval'] + 0.001, f\"{row['Average AUC']:.4f}\",\n",
    "                      ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # Add error bars to the plot\n",
    "    plt.errorbar(x=df.index, y=df['Average AUC'], yerr=df['Confidence Interval'], fmt='none', capsize=4)\n",
    "\n",
    "    plt.xlabel(\"Ensemble Configuration\")\n",
    "    plt.ylabel(\"Average Test Data AUC\")\n",
    "    plt.title(\"Average Test Data AUC Across All Datasets\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eee8af2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.750806Z",
     "start_time": "2023-07-22T10:31:52.731232Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_times_picked_model(dictionary, result_path):\n",
    "    models_times_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if \"baseline\" == ensemble_name or \"best_performer\" not in ensemble_name:\n",
    "            continue\n",
    "            \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            for model_name, model_content in ensemble_dict[\"model_dict\"].items():\n",
    "                if model_content[\"model_weight\"] == 1: \n",
    "                    if model_name not in models_times_dict: \n",
    "                        models_times_dict[model_name] = 0\n",
    "                    models_times_dict[model_name] +=1\n",
    "            #print(ensemble_dict[\"model_dict\"])\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(models_times_dict.items()), columns=['Model', 'Value'])\n",
    "\n",
    "    # Create the seaborn histogram\n",
    "    sns_plot = sns.barplot(x='Model', y='Value', data=df)\n",
    "\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Times picked as best performer\")\n",
    "    plt.title(\"Number of Times a model is the 'Best Performer'\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for index, value in enumerate(df['Value']):\n",
    "        sns_plot.text(index, value, str(value), ha='center', va='bottom')\n",
    "\n",
    "        \n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # ---------------------------- Old version -----------------------\n",
    "    #models_times_dict = {}\n",
    "    #for ensemble_name, ensemble_data in dictionary.items():\n",
    "    #    if \"baseline\" == ensemble_name:\n",
    "    #        continue\n",
    "    #    for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "    #        if \"data_ens_dict\" not in ensemble_dict:\n",
    "    #            continue\n",
    "\n",
    "    #        for model_name_key, model_data in ensemble_dict[\"data_ens_dict\"].items():\n",
    "    #            model_name = \"_\".join(model_name_key.split(\"-\")[0].split(\"_\")[1:])\n",
    "\n",
    "    #            if model_name not in models_times_dict:\n",
    "    #                models_times_dict[model_name] = 0\n",
    "    #                \n",
    "    #            if model_data[\"data_weight\"] == 1:\n",
    "    #                models_times_dict[model_name] += 1\n",
    "\n",
    "    ## Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    #model_names = list(models_times_dict.keys())\n",
    "    #times = list(models_times_dict.values())\n",
    "\n",
    "    ## Create the histogram using Seaborn's barplot\n",
    "    #sns.set(style=\"whitegrid\")\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "    #sns_plot = sns.barplot(x=model_names, y=times)\n",
    "    #plt.xlabel(\"Models\")\n",
    "    #plt.ylabel(\"Times\")\n",
    "    #plt.title(\"Times a Model is the best performing one\")\n",
    "    #plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    #\n",
    "    ## Annotate each bar with its corresponding value\n",
    "    #for index, value in enumerate(times):\n",
    "    #    plt.text(index, value, f\"{value:.2f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    #plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    #sns_plot.figure.set_dpi(500)\n",
    "    #\n",
    "    #plt.show()\n",
    "    #sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6172ef69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:53.841603Z",
     "start_time": "2023-07-22T10:31:53.828551Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_times_picked_transformation(dictionary, result_path):\n",
    "    transformation_times_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            if \"data_ens_dict\" not in ensemble_dict:\n",
    "                continue\n",
    "\n",
    "            for model_name_key, model_data in ensemble_dict[\"data_ens_dict\"].items():\n",
    "                transformation_key = model_name_key.split(\"-\")[-1]\n",
    "\n",
    "                if transformation_key not in transformation_times_dict:\n",
    "                    transformation_times_dict[transformation_key] = 0\n",
    "                    \n",
    "                if model_data[\"data_weight\"] == 1:\n",
    "                    transformation_times_dict[transformation_key] += 1\n",
    "\n",
    "    # Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    tranformations_names = list(transformation_times_dict.keys())\n",
    "    times = list(transformation_times_dict.values())\n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns_plot = sns.barplot(x=tranformations_names, y=times)\n",
    "    plt.xlabel(\"Transformation\")\n",
    "    plt.ylabel(\"Times\")\n",
    "    plt.title(\"Times a Model is the best performing one\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "    # Annotate each bar with its corresponding value\n",
    "    for index, value in enumerate(times):\n",
    "        plt.text(index, value, f\"{value:.2f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c4ee80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:54.466648Z",
     "start_time": "2023-07-22T10:31:54.449820Z"
    }
   },
   "outputs": [],
   "source": [
    "def ttest_auc_performance_compared_to_baseline(dictionary,  alpha=0.05):\n",
    "    auc_dictionary = {}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if ensemble_name not in auc_dictionary: \n",
    "            auc_dictionary[ensemble_name] = []\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            auc_dictionary[ensemble_name].append(ensemble_dict[\"auc\"]) \n",
    "\n",
    "    \n",
    "    for ensemble_name, ensemble_auc in auc_dictionary.items():\n",
    "        if \"baseline\" == ensemble_name:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        baseline_auc_values = np.array(auc_dictionary[\"baseline\"])\n",
    "        algorithm_auc_values = np.array(auc_dictionary[ensemble_name])\n",
    "\n",
    "        if len(baseline_auc_values) != len(algorithm_auc_values):\n",
    "            raise ValueError(\"Number of AUC values for the baseline and algorithm must be the same.\")\n",
    "\n",
    "        # Compute the differences between the two sets of AUC values\n",
    "        differences = algorithm_auc_values - baseline_auc_values\n",
    "\n",
    "        # Perform paired t-test\n",
    "        t_statistic, p_value = ttest_rel(algorithm_auc_values, baseline_auc_values)\n",
    "\n",
    "        # Check if the p-value is less than the significance level\n",
    "        is_significant = p_value < alpha\n",
    "\n",
    "        auc_dictionary[ensemble_name]= {\"significant\": is_significant}\n",
    "        \n",
    "    auc_dictionary.pop(\"baseline\")\n",
    "    # Create a DataFrame from the dictionary\n",
    "    \n",
    "    return auc_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844da5e",
   "metadata": {},
   "source": [
    "# PFN Ensemble Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0e3d0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:38:47.165914Z",
     "start_time": "2023-07-22T10:38:47.066774Z"
    }
   },
   "outputs": [],
   "source": [
    "class PFNEnsemble:\n",
    "    def __init__(self, \n",
    "                 model_storage_folders=[\"models_gp\"],\n",
    "                 device=\"cpu\",\n",
    "                 N_ensemble_configurations=5,\n",
    "                 verbose=False):\n",
    "        # Maps model_index to loaded model\n",
    "        self.model_dict = {}\n",
    "        self.data_ens_dict = {}\n",
    "        \n",
    "        for model_storage_folder in model_storage_folders:\n",
    "            # Iterate over all files in the model_path directory\n",
    "            for model_index, model_string in enumerate(os.listdir(model_storage_folder)):\n",
    "                if model_string.endswith('.cpkt'):  # Assuming the models have a .cpkt extension\n",
    "                    loaded_model = TabPFNClassifier(base_path=model_storage_folder,\n",
    "                                                    model_string=model_string[:-5],\n",
    "                                                    N_ensemble_configurations=N_ensemble_configurations,\n",
    "                                                    batch_size_inference=1, \n",
    "                                                    no_preprocess_mode=True)\n",
    "                    \n",
    "        \n",
    "                    self.model_dict[model_string.split('.')[0]] = {\"model\": loaded_model}\n",
    "\n",
    "    def fit(self, train_xs, train_ys):\n",
    "        for model_name, model_content in self.model_dict.items():\n",
    "            model = model_content[\"model\"]\n",
    "            model.fit(train_xs, train_ys)\n",
    "            \n",
    "    def get_split_of_train_data_simple(self, train_x, train_y, split_share):\n",
    "        temp_split_pos = int(train_x.shape[0] * split_share)\n",
    "        \n",
    "        temp_split_train_x = train_x[:temp_split_pos]\n",
    "        temp_split_test_x = train_x[temp_split_pos:]\n",
    "        \n",
    "        temp_split_train_y = train_y[:temp_split_pos]\n",
    "        temp_split_test_y = train_y[temp_split_pos:]\n",
    "        return [(temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y)]\n",
    "    \n",
    "                \n",
    "    def get_split_of_train_data_bagging(self, train_x, train_y, split_share, number_splits):\n",
    "        splits = [] # each entry has format: [(temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y)]\n",
    "        # todo repeat the following number_splits times and add each time to splits\n",
    "        for i in range(number_splits):\n",
    "            temp_split_indices = random.sample(range(len(train_x)), int(split_share * len(train_x)))\n",
    "            temp_split_train_x = [train_x[i] for i in temp_split_indices]\n",
    "            temp_split_test_x = [train_x[i] for i in range(len(train_x)) if i not in temp_split_indices]\n",
    "\n",
    "            temp_split_train_y = [train_y[i] for i in temp_split_indices]\n",
    "            temp_split_test_y = [train_y[i] for i in range(len(train_y)) if i not in temp_split_indices]\n",
    "            splits.append((temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y))\n",
    "        return splits\n",
    "    \n",
    "    \n",
    "    def get_split_of_train_data_pasting(self, train_x, train_y, split_share):\n",
    "        splits = [] # each entry has format: [(temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y)]\n",
    "        # todo repeat the following number_splits times and add each time to splits\n",
    "        list_of_indices_to_sample = list(range(len(train_x)))\n",
    "        while len(list_of_indices_to_sample) > 1: # need at least two \n",
    "            len_list_to_sample = len(list_of_indices_to_sample)\n",
    "            if len_list_to_sample <= int(split_share * len(train_x)) * 2:\n",
    "                temp_split_train_indices = list_of_indices_to_sample[:len_list_to_sample//2]\n",
    "                temp_split_test_indices = list_of_indices_to_sample[len_list_to_sample//2:]\n",
    "            else:    \n",
    "                temp_split_train_indices = random.sample(list_of_indices_to_sample, int(split_share * len(train_x)))\n",
    "                remaining_list_indices = [x for x in list_of_indices_to_sample if x not in temp_split_train_indices]\n",
    "                temp_split_test_indices = random.sample(remaining_list_indices,int(split_share * len(train_x)))\n",
    "                \n",
    "            # remove indices which has been sampled in temp_split_indices from list_of_indices_to_sample\n",
    "            list_of_indices_to_sample  = [x for x in list_of_indices_to_sample if x not in temp_split_train_indices + temp_split_test_indices]\n",
    "\n",
    "            temp_split_train_x = [train_x[i] for i in temp_split_train_indices]\n",
    "            temp_split_test_x = [train_x[i] for i in temp_split_test_indices]\n",
    "\n",
    "            temp_split_train_y = [train_y[i] for i in temp_split_train_indices]\n",
    "            temp_split_test_y = [train_y[i] for i in temp_split_test_indices]\n",
    "\n",
    "            splits.append((temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y))\n",
    "        return splits\n",
    "            \n",
    "        \n",
    "    def get_train_auc_of_model(self,model,splits): \n",
    "        total_auc = 0\n",
    "        for temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y in splits:\n",
    "            model.fit(temp_split_train_x, temp_split_train_y)\n",
    "            prediction_ = model.predict_proba(temp_split_test_x)\n",
    "            auc = tabular_metrics.auc_metric(temp_split_test_y, prediction_)\n",
    "            total_auc += auc\n",
    "        total_auc /= len(splits)\n",
    "        return total_auc\n",
    "        \n",
    "    def compute_weights(self, level, weight_type):\n",
    "        \n",
    "        if weight_type == f\"weighted_average\":\n",
    "            if level == 'data':\n",
    "                sum_of_auc = {}\n",
    "                for key in self.data_ens_dict.keys():\n",
    "                    model_name = key.split('-')[0]\n",
    "                    if model_name not in sum_of_auc.keys():\n",
    "                        sum_of_auc[model_name] = 0\n",
    "                    sum_of_auc[model_name] += self.data_ens_dict[key][f'{level}_auc']\n",
    "                    \n",
    "                for data_ens_key, data_ens_content in self.data_ens_dict.items():\n",
    "                    self.data_ens_dict[data_ens_key][f\"{level}_weight\"] = data_ens_content[f\"{level}_auc\"] / sum_of_auc[data_ens_key.split('-')[0]]\n",
    "                \n",
    "            else: # model here\n",
    "                sum_of_auc = sum([i[f'{level}_auc'] for i in self.model_dict.values()])\n",
    "\n",
    "                for ensemble_index, model_name in self.model_dict.items():\n",
    "                    self.model_dict[ensemble_index][f\"{level}_weight\"] = self.model_dict[ensemble_index][f\"{level}_auc\"] / sum_of_auc\n",
    "                \n",
    "        elif weight_type == f\"best_performer\":\n",
    "            \n",
    "            if level == 'data':\n",
    "                max_aucs = {}\n",
    "                for key in self.data_ens_dict.keys():\n",
    "                    model_name = key.split('-')[0]\n",
    "                    if model_name not in max_aucs.keys():\n",
    "                        max_aucs[model_name] = self.data_ens_dict[key][f'{level}_auc']\n",
    "                    max_aucs[model_name] = max(max_aucs[model_name], self.data_ens_dict[key][f'{level}_auc'])\n",
    "                \n",
    "                max_auc_keys = {}\n",
    "                for data_ens_key, data_ens_content in self.data_ens_dict.items():\n",
    "                    if data_ens_content[f'{level}_auc'] == max_aucs[data_ens_key.split('-')[0]]:\n",
    "                        max_auc_keys[data_ens_key.split('-')[0]] = data_ens_key\n",
    "                \n",
    "                for data_ens_key in self.data_ens_dict.keys():\n",
    "                    if data_ens_key in max_auc_keys.values():\n",
    "                        self.data_ens_dict[data_ens_key][f'{level}_weight'] = 1\n",
    "                    else:\n",
    "                        self.data_ens_dict[data_ens_key][f'{level}_weight'] = 0\n",
    "                \n",
    "            else:\n",
    "                max_model_auc = max([i[f'{level}_auc'] for i in self.model_dict.values()])\n",
    "                \n",
    "                max_auc_key = None\n",
    "                for model_key, model_content in self.model_dict.items():\n",
    "                    if model_content[f'{level}_auc'] == max_model_auc:\n",
    "                        max_auc_key = model_key\n",
    "                        break\n",
    "                \n",
    "                for model_key in self.model_dict.keys():\n",
    "                    if model_key == max_auc_key:\n",
    "                        self.model_dict[model_key][f'{level}_weight'] = 1\n",
    "                    else:\n",
    "                        self.model_dict[model_key][f'{level}_weight'] = 0\n",
    "\n",
    "        \n",
    "    def get_splits_of_train_data(self, full_train_x, full_train_y, split_type, split_share, number_splits):\n",
    "        # compute the train data splits for later AUC computation\n",
    "        if split_type == \"simple\":\n",
    "            return self.get_split_of_train_data_simple(train_x=full_train_x, train_y=full_train_y, split_share=split_share)\n",
    "        elif split_type == \"bagging\":\n",
    "            return self.get_split_of_train_data_bagging(train_x=full_train_x, train_y=full_train_y, split_share=split_share,number_splits=number_splits)\n",
    "        elif split_type == \"pasting\": \n",
    "            return self.get_split_of_train_data_pasting(train_x=full_train_x, train_y=full_train_y, split_share=split_share)\n",
    "        else:\n",
    "            print(\"todo raise error\")\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, \n",
    "                      test_xs, \n",
    "                      data_ensemble_config = None,\n",
    "                      model_ensemble_config = None,\n",
    "                      data_preprocess_config = None, \n",
    "                      multiple_models=False,\n",
    "                      pre_processing=False ):\n",
    "        \n",
    "         # ------------------- only preprocessing with one model --------------------------\n",
    "        if multiple_models is False and pre_processing is True:\n",
    "            print(\"predict_proba use only the baseline model and data-ensembling as no moe config is given! \")\n",
    "            \n",
    "            baseline_model, baseline_name = None, None\n",
    "            \n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                if \"baseline\" in model_name:\n",
    "                    baseline_model = model_content[\"model\"]\n",
    "                    baseline_name = model_name\n",
    "                    break\n",
    "    \n",
    "            if baseline_model is None:  \n",
    "                raise NotImplementedError(f'The baseline_model could not be found for predict proba.')\n",
    "\n",
    "            train_xs, train_ys = baseline_model.X_, baseline_model.y_\n",
    "            \n",
    "            # add identity\n",
    "            data_preprocess_config['sklearn_transformations'].append((\"identity\",None))\n",
    "            \n",
    "            # APPLY DATA PREPROCESSING / AUGMENTATION\n",
    "            for transformer_name, transformer in data_preprocess_config[\"sklearn_transformations\"]:\n",
    "                transformation_key = baseline_name + '-' + transformer_name\n",
    "                self.data_ens_dict[transformation_key] = {}\n",
    "                \n",
    "                if transformer_name != 'identity':\n",
    "                    train_xs_t = transformer.fit_transform(train_xs)\n",
    "                    test_xs_t = transformer.transform(test_xs)\n",
    "                else:\n",
    "                    train_xs_t = train_xs\n",
    "                    test_xs_t = test_xs\n",
    "                \n",
    "                self.data_ens_dict[transformation_key]['train_data'] = (train_xs_t, train_ys)\n",
    "                self.data_ens_dict[transformation_key]['test_data'] = test_xs_t\n",
    "                \n",
    "                splits = self.get_splits_of_train_data(full_train_x=train_xs_t,\n",
    "                                                       full_train_y=train_ys,\n",
    "                                                       split_type=data_ensemble_config[\"data_split_type\"], \n",
    "                                                       split_share=data_ensemble_config[\"data_split_share\"],\n",
    "                                                       number_splits=data_ensemble_config[\"data_number_splits\"])\n",
    "\n",
    "                # Compute AUC performance of train data\n",
    "                self.data_ens_dict[transformation_key]['data_auc'] = float(self.get_train_auc_of_model(model=baseline_model,\n",
    "                                                                                               splits=splits))\n",
    "                baseline_model.fit(train_xs_t, train_ys)\n",
    "                self.data_ens_dict[transformation_key]['test_preds'] = baseline_model.predict_proba(self.data_ens_dict[transformation_key]['test_data'])\n",
    "                \n",
    "            baseline_model.fit(train_xs, train_ys)\n",
    "            # after having collected the auc / ce /... scores of the models of the train set, we want to \n",
    "            # compute the weights for the final forward pass\n",
    "            self.compute_weights(level=\"data\",\n",
    "                                 weight_type=data_ensemble_config[\"data_weight_type\"])\n",
    "            \n",
    "            prediction_weighted = 0\n",
    "            for data_ens_key, data_ens_content in self.data_ens_dict.items():\n",
    "                prediction_weighted += data_ens_content['test_preds'].copy() * data_ens_content['data_weight']\n",
    "                \n",
    "            # Storing the weighted predictions as predictions of the model as well\n",
    "            self.model_dict[baseline_name]['test_preds'] = prediction_weighted\n",
    "            \n",
    "            return prediction_weighted\n",
    "\n",
    "            \n",
    "        # ------------------------- only model ensembling ----------------------------------------\n",
    "        elif multiple_models is True and pre_processing is False:\n",
    "            print(\"predict_proba use moe ensembling and no data-ensembling as no data preprocessing is given\")\n",
    "            \n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                \n",
    "                train_xs, train_ys = model_content['model'].X_, model_content['model'].y_\n",
    "                \n",
    "                # compute the train data splits for later AUC computation\n",
    "                splits = self.get_splits_of_train_data(full_train_x=train_xs,\n",
    "                                                       full_train_y=train_ys,\n",
    "                                                       split_type=model_ensemble_config[\"model_split_type\"], \n",
    "                                                       split_share=model_ensemble_config[\"model_split_share\"],\n",
    "                                                       number_splits=model_ensemble_config[\"model_number_splits\"])\n",
    "\n",
    "                \n",
    "                # Compute AUC performance of train data \n",
    "                self.model_dict[model_name]['model_auc'] = float(self.get_train_auc_of_model(model=model_content['model'],\n",
    "                                                                                                splits=splits))\n",
    "\n",
    "                self.model_dict[model_name]['model'].fit(train_xs, train_ys) # overwrite the training data in the model again!\n",
    "                model_content['test_preds'] = model_content['model'].predict_proba(test_xs)\n",
    "            \n",
    "            # after having collected the auc scores of the models of the train set, we want to compute the weights for the final forward pass\n",
    "            self.compute_weights(level=\"model\",\n",
    "                                 weight_type=model_ensemble_config[\"model_weight_type\"])\n",
    "            \n",
    "            prediction_weighted = 0\n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                prediction_weighted += model_content['test_preds'].copy() * model_content['model_weight']\n",
    "                          \n",
    "            return prediction_weighted\n",
    "        \n",
    "        \n",
    "        # ---------------------- both datapreprocessing for each in moe ensembling ------------------------------    \n",
    "        else: \n",
    "            print(\"data preprocessing and moe both used!\" )\n",
    "            \n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                    \n",
    "                train_xs, train_ys = model_content['model'].X_, model_content['model'].y_\n",
    "\n",
    "                # add identity\n",
    "                data_preprocess_config['sklearn_transformations'].append((\"identity\",None))\n",
    "\n",
    "                # APPLY DATA PREPROCESSING / AUGMENTATION\n",
    "                for transformer_name, transformer in data_preprocess_config[\"sklearn_transformations\"]:\n",
    "                    transformation_key = model_name + '-' + transformer_name\n",
    "                    self.data_ens_dict[transformation_key] = {}\n",
    "\n",
    "                    if transformer_name != 'identity':\n",
    "                        train_xs_t = transformer.fit_transform(train_xs)\n",
    "                        test_xs_t = transformer.transform(test_xs)\n",
    "                    else:\n",
    "                        train_xs_t = train_xs\n",
    "                        test_xs_t = test_xs\n",
    "\n",
    "                    self.data_ens_dict[transformation_key]['train_data'] = (train_xs_t, train_ys)\n",
    "                    self.data_ens_dict[transformation_key]['test_data'] = test_xs_t\n",
    "\n",
    "                    splits = self.get_splits_of_train_data(full_train_x=train_xs_t,\n",
    "                                                           full_train_y=train_ys,\n",
    "                                                           split_type=data_ensemble_config[\"data_split_type\"], \n",
    "                                                           split_share=data_ensemble_config[\"data_split_share\"],\n",
    "                                                           number_splits=data_ensemble_config[\"data_number_splits\"])\n",
    "\n",
    "                    # Compute AUC performance of train data\n",
    "                    self.data_ens_dict[transformation_key]['data_auc'] = float(self.get_train_auc_of_model(model=model_content['model'],\n",
    "                                                                                                           splits=splits))\n",
    "                    model_content['model'].fit(train_xs_t, train_ys)\n",
    "                    self.data_ens_dict[transformation_key]['test_preds'] = model_content['model'].predict_proba(self.data_ens_dict[transformation_key]['test_data'])\n",
    "\n",
    "\n",
    "                model_content['model'].fit(train_xs, train_ys)\n",
    "                # after having collected the auc scores of the models of the train set, we want to \n",
    "            \n",
    "            # compute the weights for the data ensembles\n",
    "            self.compute_weights(level=\"data\", weight_type=data_ensemble_config[\"data_weight_type\"])\n",
    "            \n",
    "            \n",
    "            for model_name in self.model_dict.keys():\n",
    "                model_prediction_weighted = 0\n",
    "                model_auc_weighted = 0\n",
    "                for data_ens_name, data_ens_content in self.data_ens_dict.items():\n",
    "                    if data_ens_name.split('-')[0] == model_name:\n",
    "                        #print(data_ens_content)\n",
    "                        model_prediction_weighted += data_ens_content['test_preds'].copy() * \\\n",
    "                                                                                    data_ens_content['data_weight']\n",
    "                        model_auc_weighted += data_ens_content['data_auc'] * \\\n",
    "                                                                                    data_ens_content['data_weight']\n",
    "                self.model_dict[model_name]['test_preds'] = model_prediction_weighted\n",
    "                self.model_dict[model_name]['model_auc'] = model_auc_weighted\n",
    "            \n",
    "            # compute the weights for the final model ensemble\n",
    "            self.compute_weights(level=\"model\",\n",
    "                                 weight_type=model_ensemble_config[\"model_weight_type\"])\n",
    "            \n",
    "            prediction_weighted = 0\n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                prediction_weighted += model_content['test_preds'] * model_content['model_weight']\n",
    "\n",
    "            return prediction_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f6ebf6",
   "metadata": {},
   "source": [
    "# Computation of all ensemble configurations with only expert-ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66b5c65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:42:33.158226Z",
     "start_time": "2023-07-22T09:41:40.794306Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset name: balance-scale size torch.Size([625, 4]) --- 0/30\n",
      "Current configuration: weighting_metric model_auc, weight_type best_performer, split_type simple\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Multiple models in memory. This might lead to memory issues. Consider calling remove_models_from_memory()\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "Loading model that can be used for inference only\n",
      "Using a Transformer with 25.82 M parameters\n",
      "predict_proba use moe ensembling and no data-ensembling as no data preprocessing is given\n",
      "Current configuration: weighting_metric model_auc, weight_type best_performer, split_type bagging\n",
      "predict_proba use moe ensembling and no data-ensembling as no data preprocessing is given\n",
      "Current configuration: weighting_metric model_auc, weight_type best_performer, split_type pasting\n",
      "predict_proba use moe ensembling and no data-ensembling as no data preprocessing is given\n",
      "Current configuration: weighting_metric model_auc, weight_type weighted_average, split_type simple\n",
      "predict_proba use moe ensembling and no data-ensembling as no data preprocessing is given\n",
      "Current configuration: weighting_metric model_auc, weight_type weighted_average, split_type bagging\n",
      "predict_proba use moe ensembling and no data-ensembling as no data preprocessing is given\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2a06b6433cd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m                                                                     \u001b[0mdata_preprocess_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_preprocess_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                                                                     \u001b[0mmultiple_models\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                                                                     pre_processing=False)\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[0mauc_ensemble\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtabular_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauc_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-82a7d5e8657f>\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, test_xs, data_ensemble_config, model_ensemble_config, data_preprocess_config, multiple_models, pre_processing)\u001b[0m\n\u001b[0;32m    247\u001b[0m                 \u001b[1;31m# Compute AUC performance of train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m                 self.model_dict[model_name]['model_auc'] = float(self.get_train_auc_of_model(model=model_content['model'],\n\u001b[1;32m--> 249\u001b[1;33m                                                                                                 splits=splits))\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ys\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# overwrite the training data in the model again!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-82a7d5e8657f>\u001b[0m in \u001b[0;36mget_train_auc_of_model\u001b[1;34m(self, model, splits)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_split_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_split_test_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_split_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_split_test_y\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_split_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_split_train_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mprediction_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_split_test_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtabular_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauc_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_split_test_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mtotal_auc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\Desktop\\MSGPFN\\tabpfn\\scripts\\transformer_prediction_interface.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, normalize_with_test, return_logits)\u001b[0m\n\u001b[0;32m    289\u001b[0m                                          \u001b[0mno_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                                          \u001b[0mbatch_size_inference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size_inference\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m                                          **get_params_from_config(self.c))\n\u001b[0m\u001b[0;32m    292\u001b[0m         prediction_, y_ = prediction.squeeze(\n\u001b[0;32m    293\u001b[0m             0), y_full.squeeze(1).long()[eval_pos:]\n",
      "\u001b[1;32mC:\\Users\\giaco\\Desktop\\MSGPFN\\tabpfn\\scripts\\transformer_prediction_interface.py\u001b[0m in \u001b[0;36mtransformer_predict\u001b[1;34m(model, eval_xs, eval_ys, eval_position, device, max_features, style, inference_mode, num_classes, extend_features, normalize_with_test, normalize_to_ranking, softmax_temperature, multiclass_decoder, preprocess_transform, categorical_feats, feature_shift_decoder, N_ensemble_configurations, batch_size_inference, differentiable_hps_as_style, average_logits, fp16_inference, normalize_with_sqrt, seed, no_grad, return_logits, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                 output_batch = checkpoint(\n\u001b[1;32m--> 546\u001b[1;33m                     predict, batch_input, batch_label, style_, softmax_temperature_, True)\n\u001b[0m\u001b[0;32m    547\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfp16_inference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\utils\\checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[1;34m(function, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unexpected keyword arguments: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\utils\\checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\Desktop\\MSGPFN\\tabpfn\\scripts\\transformer_prediction_interface.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(eval_xs, eval_ys, used_style, softmax_temperature, return_logits)\u001b[0m\n\u001b[0;32m    366\u001b[0m                 (used_style.repeat(\n\u001b[0;32m    367\u001b[0m                     eval_xs.shape[1], 1) if used_style is not None else None, eval_xs, eval_ys.float()),\n\u001b[1;32m--> 368\u001b[1;33m                 single_eval_pos=eval_position)[:, :, 0:num_classes]\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\tabpfn\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, single_eval_pos)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msingle_eval_pos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_src\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_att_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_att_embeddings\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\tabpfn\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\tabpfn\\layer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0msingle_eval_position\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0msrc_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0msrc_right\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0msrc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msrc_left\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_right\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[0;32m   1036\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m                 attn_mask=attn_mask)\n\u001b[0m\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[0;32m   5082\u001b[0m     \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5083\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5084\u001b[1;33m     \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5085\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5086\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\giaco\\miniconda3\\envs\\msgpfn\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluations_folder = \"./evaluations/experts/\"\n",
    "file_name = f\"{evaluations_folder}only_experts_ensemble_results.pickle\"\n",
    "\n",
    "\n",
    "model_paths = [\"./models_gp\",\"./models_causal\", \"./models_bag\"]#,\"./baseline\"]    \n",
    "    \n",
    "baseline_classifier = TabPFNClassifier(device=device, \n",
    "                                     base_path=\"./baseline\",\n",
    "                                     model_string=\"model_bag_baseline\",\n",
    "                                     N_ensemble_configurations=1,\n",
    "                                     batch_size_inference=1, \n",
    "                                     no_preprocess_mode=True)\n",
    "\n",
    "# Ensemble configuration \n",
    "\n",
    "model_weighting_metrics = [\"model_auc\"]\n",
    "model_weight_types = [\"best_performer\",\"weighted_average\"]\n",
    "model_split_types =[\"simple\", \"bagging\", \"pasting\"]\n",
    "\n",
    "\n",
    "# Configure run \n",
    "ensemble_results = {}\n",
    "\n",
    "# Indices of the datasets to predict\n",
    "start = 0\n",
    "end =  2\n",
    "number_iterations = range(max(start, 0),min(end,len(test_datasets)))\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        ensemble_results = pickle.load(file)\n",
    "else:\n",
    "    for dataset_index in number_iterations:\n",
    "\n",
    "        train_xs, train_ys, test_xs, test_ys, dataset_name = prepare_dataset_for_evaluation(dataset_index=dataset_index, test_datasets=test_datasets)\n",
    "\n",
    "        # compare to normal tabpfn\n",
    "        baseline_classifier.fit(train_xs, train_ys)\n",
    "        prediction_tabpfn = baseline_classifier.predict_proba(test_xs)\n",
    "        auc_baseline = tabular_metrics.auc_metric(test_ys, prediction_tabpfn)\n",
    "\n",
    "        # add computed results to ensemble_results\n",
    "        if \"baseline\" not in ensemble_results:\n",
    "            ensemble_results[\"baseline\"] = {}\n",
    "\n",
    "        if dataset_name not in ensemble_results[\"baseline\"]:\n",
    "            ensemble_results[\"baseline\"][dataset_name] = {}\n",
    "\n",
    "\n",
    "        baseline_dict = {\"auc\":auc_baseline.item()}\n",
    "        ensemble_results[\"baseline\"][dataset_name]= baseline_dict\n",
    "\n",
    "\n",
    "        for weighting_metric in model_weighting_metrics: \n",
    "            for weight_type in model_weight_types: \n",
    "                for split_type in model_split_types:\n",
    "                    print(f\"Current configuration: weighting_metric {weighting_metric}, weight_type {weight_type}, split_type {split_type}\")\n",
    "\n",
    "\n",
    "                    model_ensemble_config = {\"model_weighting_metric\":weighting_metric,\n",
    "                                             \"model_weight_type\":weight_type,\n",
    "                                             \"model_split_type\":split_type,\n",
    "                                             \"model_split_share\":0.5, \n",
    "                                             \"model_number_splits\": 5}\n",
    "\n",
    "                    data_preprocess_config = None\n",
    "                    data_ensemble_config = None\n",
    "\n",
    "                    # create classifier with configuration \n",
    "                    classifier_ensemble = PFNEnsemble(model_storage_folders=model_paths, N_ensemble_configurations=1, device=device)\n",
    "\n",
    "                    classifier_ensemble.fit(train_xs, train_ys)\n",
    "                    prediction_ = classifier_ensemble.predict_proba(test_xs,\n",
    "                                                                    data_ensemble_config=data_ensemble_config,\n",
    "                                                                    model_ensemble_config=model_ensemble_config,\n",
    "                                                                    data_preprocess_config=data_preprocess_config, \n",
    "                                                                    multiple_models=True,\n",
    "                                                                    pre_processing=False)\n",
    "\n",
    "                    auc_ensemble = tabular_metrics.auc_metric(test_ys, prediction_)\n",
    "                    \n",
    "                    # add computed results to results dict\n",
    "                    ensemble_name = f\"{weight_type}-{split_type}\"\n",
    "\n",
    "                    if ensemble_name not in ensemble_results:\n",
    "                        ensemble_results[ensemble_name] = {}\n",
    "\n",
    "                    if dataset_name not in ensemble_results[ensemble_name]:\n",
    "                        ensemble_results[ensemble_name][dataset_name] = {}\n",
    "                        \n",
    "                    ensemble_dict = {\"auc\":auc_ensemble.item(),\n",
    "                                    \"model_dict\":classifier_ensemble.model_dict,\n",
    "                                    \"data_ens_dict\":classifier_ensemble.data_ens_dict}\n",
    "\n",
    "                    ensemble_results[ensemble_name][dataset_name]= ensemble_dict\n",
    "                    \n",
    "    save_dict_to_pickle_file(ensemble_results, file_name, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c01ef7",
   "metadata": {},
   "source": [
    "## ONLY EXPERT EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d770165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:40:01.938414Z",
     "start_time": "2023-07-22T09:40:01.910265Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#---------------------------- S T A R T - E V A L U A T I O N - P L O T S ----------------------------\\nensemble_results = clean_result_dictionary_for_evaluation(ensemble_results, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])\\n#---------------------------- H E A T M A P S ----------------------------\\nstart, stop = 0, 15\\n\\ndataset_names = [dataset[0] for dataset_index,dataset in enumerate(test_datasets) if start <= dataset_index <= stop]\\nplot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, f\"{evaluations_folder}dataset[{start}-{stop}]-plot_heatmap_auc_datasets_configs.png\" )\\n\\n\\n#---------------------------- A V E R A G E - W EI G H T - H I S T O G R A M -------------------------\\n#transformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \\n#plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\\n\\n\\n#---------------------------- S C A T T E R -------------------------\\n#plot_scatter_train_auc_test_auc_best_performer_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_best_performer_transformations.png\")\\n#plot_scatter_train_auc_test_auc_weighted_average_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_weighted_average_transformations.png\")\\n\\n# TODO \\n\\n#------------------------------------------------------------------------------------------------------------\\n#---------------------------- T R A N S F O R M A T I O N S - H I S T O G R A M  ----------------------------\\n#transformation_names =[transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \\n#plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\\n\\n\\n#------------------------------------------------------------------------------------------------------------\\n#---------------------------- I N S T A N C E S - A U C - H I S T O G R A M  --------------------------------\\nplot_histogram_average_test_AUC_instance(ensemble_results, f\"{evaluations_folder}plot_histogram_average_test_AUC_instance.png\")\\n\\n\\n#------------------------------------------------------------------------------------------------------------\\n#---------------------------- M O D E L - A U C - H I S T O G R A M  ----------------------------------------\\nplot_histogram_average_train_auc_model(ensemble_results, f\"{evaluations_folder}plot_histogram_average_train_auc_model.png\" )\\n\\n\\n#------------------------------------------------------------------------------------------------------------\\n#---------------------------- T I M E S - M O D E L - H I S T O G R A M  ------------------------------------\\nplot_histogram_times_picked_model(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_model.png\" )\\n\\n\\n#--------------------------------------------------------------------------------------------------\\n#------------------ T I M E S - T R A N S F O R M A T I O N - H I S T O G R A M  ------------------\\n# plot_histogram_times_picked_transformation(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_transformation.png\" )\\n\\n\\n#---------------------------- S T A T I S T I C A L - T E S T S -------------------------\\nttest_auc_performance_compared_to_baseline(dictionary=ensemble_results, alpha=0.05)\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "#---------------------------- S T A R T - E V A L U A T I O N - P L O T S ----------------------------\n",
    "ensemble_results = clean_result_dictionary_for_evaluation(ensemble_results, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])\n",
    "#---------------------------- H E A T M A P S ----------------------------\n",
    "start, stop = 0, 15\n",
    "\n",
    "dataset_names = [dataset[0] for dataset_index,dataset in enumerate(test_datasets) if start <= dataset_index <= stop]\n",
    "plot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, f\"{evaluations_folder}dataset[{start}-{stop}]-plot_heatmap_auc_datasets_configs.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- A V E R A G E - W EI G H T - H I S T O G R A M -------------------------\n",
    "#transformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \n",
    "#plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- S C A T T E R -------------------------\n",
    "#plot_scatter_train_auc_test_auc_best_performer_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_best_performer_transformations.png\")\n",
    "#plot_scatter_train_auc_test_auc_weighted_average_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_weighted_average_transformations.png\")\n",
    "\n",
    "# TODO \n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- T R A N S F O R M A T I O N S - H I S T O G R A M  ----------------------------\n",
    "#transformation_names =[transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \n",
    "#plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- I N S T A N C E S - A U C - H I S T O G R A M  --------------------------------\n",
    "plot_histogram_average_test_AUC_instance(ensemble_results, f\"{evaluations_folder}plot_histogram_average_test_AUC_instance.png\")\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- M O D E L - A U C - H I S T O G R A M  ----------------------------------------\n",
    "plot_histogram_average_train_auc_model(ensemble_results, f\"{evaluations_folder}plot_histogram_average_train_auc_model.png\" )\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- T I M E S - M O D E L - H I S T O G R A M  ------------------------------------\n",
    "plot_histogram_times_picked_model(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_model.png\" )\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#------------------ T I M E S - T R A N S F O R M A T I O N - H I S T O G R A M  ------------------\n",
    "# plot_histogram_times_picked_transformation(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_transformation.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- S T A T I S T I C A L - T E S T S -------------------------\n",
    "ttest_auc_performance_compared_to_baseline(dictionary=ensemble_results, alpha=0.05)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c09a4f",
   "metadata": {},
   "source": [
    "# Computation of all ensemble configurations with only data-ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cfa69c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:40:02.234453Z",
     "start_time": "2023-07-22T09:40:01.941414Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# C O N F I G U R A T I O N \n",
    "model_paths = [\"./baseline\"]\n",
    "\n",
    "evaluations_folder = \"./evaluations/data/\"\n",
    "\n",
    "file_name = f\"{evaluations_folder}only_data_ensemble_results.pickle\"\n",
    "\n",
    "baseline_classifier = TabPFNClassifier(device=device, base_path=\"./baseline\", model_string=\"model_bag_baseline\",\n",
    "                                     N_ensemble_configurations=1, batch_size_inference=1, no_preprocess_mode=True,\n",
    "                                     multiclass_decoder='permutation', feature_shift_decoder=False)\n",
    "\n",
    "# Ensemble configuration \n",
    "weighting_metrics = [\"data_auc\"]\n",
    "weight_types = [\"best_performer\",\"weighted_average\"]\n",
    "split_types =[\"simple\", \"bagging\", \"pasting\"]\n",
    "\n",
    "# Data preprocessing configuration\n",
    "data_preprocess_config = {\"N_ensemble_configurations\": 1,\n",
    "                          \"sklearn_transformations\": [(\"PCA\",PCA()),\n",
    "                                                      (\"KernelPCA\",KernelPCA()),\n",
    "                                                      (\"TruncatedSVD\",TruncatedSVD()),\n",
    "                                                      (\"FastICA\",FastICA()),\n",
    "                                                      (\"FeatureAgglomeration\",FeatureAgglomeration()),\n",
    "                                                      (\"RobustScaler\",RobustScaler()),\n",
    "                                                      (\"PowerTransformer\",PowerTransformer()),\n",
    "                                                      (\"QuantileTransformer\",QuantileTransformer(n_quantiles=100))\n",
    "                                                     ]\n",
    "                         }\n",
    "\n",
    "# Configure run \n",
    "ensemble_results = {}\n",
    "\n",
    "# Indices of the datasets to predict\n",
    "start = 0\n",
    "end =  2\n",
    "number_iterations = range(max(start, 0),min(end,len(test_datasets)))\n",
    "\n",
    "    \n",
    "#---------------------------- S T A R T - E V A L U A T I O N - R U N ----------------------------\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        ensemble_results = pickle.load(file)\n",
    "else:\n",
    "    for dataset_index in number_iterations:\n",
    "        train_xs, train_ys, test_xs, test_ys, dataset_name = prepare_dataset_for_evaluation(dataset_index=dataset_index, test_datasets=test_datasets)\n",
    "        \n",
    "        \n",
    "        # compare to baseline tabpfn\n",
    "        baseline_classifier.fit(train_xs, train_ys)\n",
    "        prediction_tabpfn = baseline_classifier.predict_proba(test_xs)\n",
    "        auc_baseline = tabular_metrics.auc_metric(test_ys, prediction_tabpfn)\n",
    "\n",
    "        # add computed results to ensemble_results\n",
    "        if \"baseline\" not in ensemble_results:\n",
    "            ensemble_results[\"baseline\"] = {}\n",
    "\n",
    "        if dataset_name not in ensemble_results[\"baseline\"]:\n",
    "            ensemble_results[\"baseline\"][dataset_name] = {}\n",
    "\n",
    "\n",
    "        baseline_dict = {\"auc\":auc_baseline.item()}\n",
    "        ensemble_results[\"baseline\"][dataset_name]= baseline_dict\n",
    "\n",
    "        # Ensemble Configurations \n",
    "        for weighting_metric in weighting_metrics: \n",
    "            for weight_type in weight_types: \n",
    "                for split_type in split_types:\n",
    "                    print(f\"Current configuration: weighting_metric {weighting_metric}, weight_type {weight_type}, split_type {split_type}\")\n",
    "\n",
    "                    data_ensemble_config = {\"data_weighting_metric\":weighting_metric,\n",
    "                                            \"data_weight_type\":weight_type,\n",
    "                                            \"data_split_type\":split_type,\n",
    "                                            \"data_split_share\":0.5, \n",
    "                                            \"data_number_splits\": 3}\n",
    "\n",
    "                    \n",
    "                    # create classifier with configuration \n",
    "                    classifier_ensemble = PFNEnsemble(model_storage_folders=model_paths,\n",
    "                                                      N_ensemble_configurations=data_preprocess_config[\"N_ensemble_configurations\"],\n",
    "                                                      device=device)\n",
    "\n",
    "                    classifier_ensemble.fit(train_xs, train_ys)\n",
    "                    prediction_ = classifier_ensemble.predict_proba(test_xs,\n",
    "                                                                    model_ensemble_config= None,\n",
    "                                                                    data_ensemble_config=data_ensemble_config,\n",
    "                                                                    data_preprocess_config=data_preprocess_config,\n",
    "                                                                    multiple_models=False,\n",
    "                                                                    pre_processing=True )\n",
    "\n",
    "                    auc_ensemble  = tabular_metrics.auc_metric(test_ys, prediction_)\n",
    "\n",
    "                    # add computed results to results dict\n",
    "                    ensemble_name = f\"{weight_type}-{split_type}\"\n",
    "\n",
    "                    if ensemble_name not in ensemble_results:\n",
    "                        ensemble_results[ensemble_name] = {}\n",
    "\n",
    "                    if dataset_name not in ensemble_results[ensemble_name]:\n",
    "                        ensemble_results[ensemble_name][dataset_name] = {}\n",
    "                        \n",
    "                    ensemble_dict = {\"auc\":auc_ensemble.item(),\n",
    "                                    \"model_dict\":classifier_ensemble.model_dict,\n",
    "                                    \"data_ens_dict\":classifier_ensemble.data_ens_dict}\n",
    "\n",
    "                    ensemble_results[ensemble_name][dataset_name]= ensemble_dict\n",
    "                    \n",
    "    save_dict_to_pickle_file(ensemble_results, file_name, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "919cea89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:40:02.250657Z",
     "start_time": "2023-07-22T09:40:02.237983Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#---------------------------- S T A R T - E V A L U A T I O N - P L O T S ----------------------------\\n\\n#---------------------------- H E A T M A P S ----------------------------\\nstart, stop = 0, 5\\n\\ndataset_names = [dataset[0] for dataset_index,dataset in enumerate(test_datasets) if start <= dataset_index <= stop]\\nplot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, f\"{evaluations_folder}dataset[{start}-{stop}]-plot_heatmap_auc_datasets_configs.png\" )\\n\\n\\n#---------------------------- A V E R A G E - W EI G H T - H I S T O G R A M -------------------------\\ntransformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \\nplot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\\n\\n\\n#---------------------------- S C A T T E R -------------------------\\nplot_scatter_train_auc_test_auc_best_performer_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_best_performer_transformations.png\")\\nplot_scatter_train_auc_test_auc_weighted_average_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_weighted_average_transformations.png\")\\n\\n\\n#------------------------------------------------------------------------------------------------------------\\n#---------------------------- T R A N S F O R M A T I O N S - H I S T O G R A M  ----------------------------\\ntransformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \\nplot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\\n\\n\\n#------------------------------------------------------------------------------------------------------------\\n#---------------------------- I N S T A N C E S - A U C - H I S T O G R A M  --------------------------------\\nplot_histogram_average_test_AUC_instance(ensemble_results, f\"{evaluations_folder}plot_histogram_average_test_AUC_instance.png\")\\n\\n\\n#------------------------------------------------------------------------------------------------------------\\n#---------------------------- M O D E L - A U C - H I S T O G R A M  ----------------------------------------\\n##### DOES NOT MAKE SENSE IN ONLY DATA ENSEMBLING\\n# plot_histogram_average_train_auc_model(ensemble_results, f\"{evaluations_folder}plot_histogram_average_train_auc_model.png\" )\\n\\n\\n#------------------------------------------------------------------------------------------------------------\\n#---------------------------- T I M E S - M O D E L - H I S T O G R A M  ------------------------------------\\n##### DOES NOT MAKE SENSE IN ONLY DATA ENSEMBLING\\n# plot_histogram_times_picked_model(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_model.png\" )\\n\\n\\n#--------------------------------------------------------------------------------------------------\\n#------------------ T I M E S - T R A N S F O R M A T I O N - H I S T O G R A M  ------------------\\nplot_histogram_times_picked_transformation(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_transformation.png\" )\\n\\n\\n#---------------------------- S T A T I S T I C A L - T E S T S -------------------------\\nttest_auc_performance_compared_to_baseline(dictionary=ensemble_results,alpha=0.05)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#---------------------------- S T A R T - E V A L U A T I O N - P L O T S ----------------------------\n",
    "\n",
    "#---------------------------- H E A T M A P S ----------------------------\n",
    "start, stop = 0, 5\n",
    "\n",
    "dataset_names = [dataset[0] for dataset_index,dataset in enumerate(test_datasets) if start <= dataset_index <= stop]\n",
    "plot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, f\"{evaluations_folder}dataset[{start}-{stop}]-plot_heatmap_auc_datasets_configs.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- A V E R A G E - W EI G H T - H I S T O G R A M -------------------------\n",
    "transformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \n",
    "plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- S C A T T E R -------------------------\n",
    "plot_scatter_train_auc_test_auc_best_performer_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_best_performer_transformations.png\")\n",
    "plot_scatter_train_auc_test_auc_weighted_average_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_weighted_average_transformations.png\")\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- T R A N S F O R M A T I O N S - H I S T O G R A M  ----------------------------\n",
    "transformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \n",
    "plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- I N S T A N C E S - A U C - H I S T O G R A M  --------------------------------\n",
    "plot_histogram_average_test_AUC_instance(ensemble_results, f\"{evaluations_folder}plot_histogram_average_test_AUC_instance.png\")\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- M O D E L - A U C - H I S T O G R A M  ----------------------------------------\n",
    "##### DOES NOT MAKE SENSE IN ONLY DATA ENSEMBLING\n",
    "# plot_histogram_average_train_auc_model(ensemble_results, f\"{evaluations_folder}plot_histogram_average_train_auc_model.png\" )\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- T I M E S - M O D E L - H I S T O G R A M  ------------------------------------\n",
    "##### DOES NOT MAKE SENSE IN ONLY DATA ENSEMBLING\n",
    "# plot_histogram_times_picked_model(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_model.png\" )\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#------------------ T I M E S - T R A N S F O R M A T I O N - H I S T O G R A M  ------------------\n",
    "plot_histogram_times_picked_transformation(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_transformation.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- S T A T I S T I C A L - T E S T S -------------------------\n",
    "ttest_auc_performance_compared_to_baseline(dictionary=ensemble_results,alpha=0.05)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f956792",
   "metadata": {},
   "source": [
    "# Combined Data and Expert Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b7c9a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:39:42.070168Z",
     "start_time": "2023-07-22T10:38:53.080317Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset name: balance-scale size torch.Size([625, 4]) --- 0/30\n",
      "Current model_configuration: model_weighting_metric model_auc, model_weight_type best_performer, model_split_type simple\n",
      "data preprocessing and moe both used!\n",
      "0.967012987012987\n",
      "0.9552698412698413\n",
      "0.5931053391053391\n",
      "0.9703520923520923\n",
      "0.6494862914862916\n",
      "0.9717691197691197\n",
      "0.975012987012987\n",
      "0.9744906204906205\n",
      "0.9717691197691197\n",
      "data preprocessing and moe both used!\n",
      "0.9664386256537998\n",
      "0.952743028233003\n",
      "0.5892614508127652\n",
      "0.9627043099064349\n",
      "0.6180719615511094\n",
      "0.9691793428699862\n",
      "0.9741825082060774\n",
      "0.9773041918255263\n",
      "0.9717808764196277\n",
      "0.9769800041330867\n",
      "data preprocessing and moe both used!\n",
      "0.967012987012987\n",
      "0.9552698412698413\n",
      "0.5931053391053391\n",
      "0.9701673881673881\n",
      "0.6494862914862916\n",
      "0.9717691197691197\n",
      "0.975012987012987\n",
      "0.9744906204906205\n",
      "0.9717691197691197\n",
      "0.9717691197691197\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1940\\1851811476.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m                                                                                     \u001b[0mdata_preprocess_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_preprocess_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                                                                                     \u001b[0mmultiple_models\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m                                                                                     pre_processing=True)\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                                     \u001b[0mauc_ensemble\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtabular_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauc_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1940\\502977777.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, test_xs, data_ensemble_config, model_ensemble_config, data_preprocess_config, multiple_models, pre_processing)\u001b[0m\n\u001b[0;32m    297\u001b[0m                     \u001b[1;31m# Compute AUC performance of train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                     self.data_ens_dict[transformation_key]['data_auc'] = float(self.get_train_auc_of_model(model=model_content['model'],\n\u001b[1;32m--> 299\u001b[1;33m                                                                                                            splits=splits))\n\u001b[0m\u001b[0;32m    300\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_ens_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtransformation_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_auc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mmodel_content\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_xs_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_ys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1940\\502977777.py\u001b[0m in \u001b[0;36mget_train_auc_of_model\u001b[1;34m(self, model, splits)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_split_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_split_test_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_split_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_split_test_y\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_split_train_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_split_train_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             \u001b[0mprediction_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_split_test_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m             \u001b[0mauc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtabular_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauc_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_split_test_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mtotal_auc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\uni\\su_se_23\\dl_lab\\MSGPFN\\tabpfn\\scripts\\transformer_prediction_interface.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X, normalize_with_test, return_logits)\u001b[0m\n\u001b[0;32m    289\u001b[0m                                          \u001b[0mno_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                                          \u001b[0mbatch_size_inference\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size_inference\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m                                          **get_params_from_config(self.c))\n\u001b[0m\u001b[0;32m    292\u001b[0m         prediction_, y_ = prediction.squeeze(\n\u001b[0;32m    293\u001b[0m             0), y_full.squeeze(1).long()[eval_pos:]\n",
      "\u001b[1;32m~\\uni\\su_se_23\\dl_lab\\MSGPFN\\tabpfn\\scripts\\transformer_prediction_interface.py\u001b[0m in \u001b[0;36mtransformer_predict\u001b[1;34m(model, eval_xs, eval_ys, eval_position, device, max_features, style, inference_mode, num_classes, extend_features, normalize_with_test, normalize_to_ranking, softmax_temperature, multiclass_decoder, preprocess_transform, categorical_feats, feature_shift_decoder, N_ensemble_configurations, batch_size_inference, differentiable_hps_as_style, average_logits, fp16_inference, normalize_with_sqrt, seed, no_grad, return_logits, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m                 output_batch = checkpoint(\n\u001b[1;32m--> 546\u001b[1;33m                     predict, batch_input, batch_label, style_, softmax_temperature_, True)\n\u001b[0m\u001b[0;32m    547\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfp16_inference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lab_env\\lib\\site-packages\\torch\\utils\\checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[1;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_reentrant\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         return _checkpoint_without_reentrant(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lab_env\\lib\\site-packages\\torch\\utils\\checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\uni\\su_se_23\\dl_lab\\MSGPFN\\tabpfn\\scripts\\transformer_prediction_interface.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(eval_xs, eval_ys, used_style, softmax_temperature, return_logits)\u001b[0m\n\u001b[0;32m    366\u001b[0m                 (used_style.repeat(\n\u001b[0;32m    367\u001b[0m                     eval_xs.shape[1], 1) if used_style is not None else None, eval_xs, eval_ys.float()),\n\u001b[1;32m--> 368\u001b[1;33m                 single_eval_pos=eval_position)[:, :, 0:num_classes]\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lab_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\uni\\su_se_23\\dl_lab\\MSGPFN\\tabpfn\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, single_eval_pos)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msingle_eval_pos\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_src\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_att_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_att_embeddings\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lab_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\uni\\su_se_23\\dl_lab\\MSGPFN\\tabpfn\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lab_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\uni\\su_se_23\\dl_lab\\MSGPFN\\tabpfn\\layer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[0msrc_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0msrc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lab_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\lab_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "evaluations_folder = \"./evaluations/combined/\"\n",
    "file_name = f\"{evaluations_folder}combined_data_and_experts_ensemble_results.pickle\"\n",
    "\n",
    "\n",
    "model_paths = [\"./models_gp\",\"./models_causal\",\"./models_bag\"]    \n",
    "\n",
    "baseline_classifier = TabPFNClassifier(device=device, \n",
    "                                     base_path=\"./baseline\",\n",
    "                                     model_string=\"model_bag_baseline\",\n",
    "                                     N_ensemble_configurations=1,\n",
    "                                     batch_size_inference=1, \n",
    "                                     no_preprocess_mode=True,\n",
    "                                     multiclass_decoder='permutation',\n",
    "                                     feature_shift_decoder=False)#??? \n",
    "\n",
    "data_weighting_metrics = [\"data_auc\"]  \n",
    "data_weight_types = [\"best_performer\",\"weighted_average\"] \n",
    "data_split_types =[\"simple\", \"bagging\", \"pasting\"] \n",
    "\n",
    "model_weighting_metrics = [\"model_auc\"]  \n",
    "model_weight_types = [\"best_performer\",\"weighted_average\"] \n",
    "model_split_types =[\"simple\", \"bagging\", \"pasting\"] \n",
    "\n",
    "data_preprocess_config = {\"N_ensemble_configurations\": 1,\n",
    "                          \"sklearn_transformations\": [(\"PCA\",PCA()),\n",
    "                                                      (\"KernelPCA\",KernelPCA()),\n",
    "                                                      (\"TruncatedSVD\",TruncatedSVD()),\n",
    "                                                      (\"FastICA\",FastICA()),\n",
    "                                                      (\"FeatureAgglomeration\",FeatureAgglomeration()),\n",
    "                                                      (\"RobustScaler\",RobustScaler()),\n",
    "                                                      (\"PowerTransformer\",PowerTransformer()),\n",
    "                                                      (\"QuantileTransformer\",QuantileTransformer(n_quantiles=100))\n",
    "                                                     ]\n",
    "                         }\n",
    "\n",
    "# Configure run \n",
    "ensemble_results = {}\n",
    "\n",
    "start = 0\n",
    "end = 2\n",
    "number_iterations = range(max(start, 0),min(end,len(test_datasets)))# Index of the dataset to predict\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        ensemble_results = pickle.load(file)\n",
    "else:\n",
    "    for evaluation_dataset_index in number_iterations:\n",
    "        train_xs, train_ys, test_xs, test_ys, dataset_name = prepare_dataset_for_evaluation(dataset_index=evaluation_dataset_index, test_datasets=test_datasets)\n",
    "        \n",
    "        # compare to baseline tabpfn\n",
    "        baseline_classifier.fit(train_xs, train_ys)\n",
    "        prediction_tabpfn = baseline_classifier.predict_proba(test_xs)\n",
    "        auc_baseline = tabular_metrics.auc_metric(test_ys, prediction_tabpfn)\n",
    "\n",
    "        # add computed results to ensemble_results\n",
    "        if \"baseline\" not in ensemble_results:\n",
    "            ensemble_results[\"baseline\"] = {}\n",
    "\n",
    "        if dataset_name not in ensemble_results[\"baseline\"]:\n",
    "            ensemble_results[\"baseline\"][dataset_name] = {}\n",
    "\n",
    "\n",
    "        baseline_dict = {\"auc\":auc_baseline.item()}\n",
    "        ensemble_results[\"baseline\"][dataset_name]= baseline_dict\n",
    "        \n",
    "        \n",
    "        # MoE Configurations ---------------------\n",
    "        for model_weighting_metric in model_weighting_metrics: \n",
    "            for model_weight_type in model_weight_types: \n",
    "                for model_split_type in model_split_types:\n",
    "                    print(f\"Current model_configuration: model_weighting_metric {model_weighting_metric}, model_weight_type {model_weight_type}, model_split_type {model_split_type}\")\n",
    "                    # --------------------------------\n",
    "                    model_ensemble_config = {\"model_weighting_metric\":model_weighting_metric,\n",
    "                                             \"model_weight_type\":model_weight_type,\n",
    "                                             \"model_split_type\":model_split_type,\n",
    "                                             \"model_split_share\":0.5, \n",
    "                                             \"model_number_splits\": 5}\n",
    "\n",
    "             \n",
    "                    # create classifier with configuration \n",
    "                    classifier_ensemble = PFNEnsemble(model_storage_folders=model_paths,\n",
    "                                               N_ensemble_configurations=data_preprocess_config[\"N_ensemble_configurations\"],\n",
    "                                               device=device)\n",
    "\n",
    "                    for data_weighting_metric in data_weighting_metrics: \n",
    "                            for data_weight_type in data_weight_types: \n",
    "                                for data_split_type in data_split_types:\n",
    "\n",
    "                                    data_ensemble_config = {\"data_weighting_metric\":data_weighting_metric,\n",
    "                                                            \"data_weight_type\":data_weight_type,\n",
    "                                                            \"data_split_type\":data_split_type,\n",
    "                                                            \"data_split_share\":0.5, \n",
    "                                                            \"data_number_splits\": 5}\n",
    "\n",
    "                                    classifier_ensemble.fit(train_xs, train_ys)\n",
    "                                    prediction_ = classifier_ensemble.predict_proba(test_xs,\n",
    "                                                                                    model_ensemble_config=model_ensemble_config,\n",
    "                                                                                    data_ensemble_config=data_ensemble_config,\n",
    "                                                                                    data_preprocess_config=data_preprocess_config,\n",
    "                                                                                    multiple_models=True,\n",
    "                                                                                    pre_processing=True)\n",
    "\n",
    "                                    auc_ensemble = tabular_metrics.auc_metric(test_ys, prediction_)\n",
    "                                    \n",
    "                                    # add computed results to results dict\n",
    "                                    ensemble_name = f\"m_{model_weight_type}-m_{model_split_type}-d_{data_weight_type}-d_{data_split_type}\"\n",
    "\n",
    "                                    if ensemble_name not in ensemble_results:\n",
    "                                        ensemble_results[ensemble_name] = {}\n",
    "\n",
    "                                    if dataset_name not in ensemble_results[ensemble_name]:\n",
    "                                        ensemble_results[ensemble_name][dataset_name] = {}\n",
    "\n",
    "                                    ensemble_dict = {\"auc\":auc_ensemble.item(),\n",
    "                                                    \"model_dict\":classifier_ensemble.model_dict,\n",
    "                                                    \"data_ens_dict\":classifier_ensemble.data_ens_dict}\n",
    "\n",
    "                                    ensemble_results[ensemble_name][dataset_name]= ensemble_dict\n",
    "\n",
    "    save_dict_to_pickle_file(ensemble_results, file_name, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a6910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:40:02.472725Z",
     "start_time": "2023-07-22T09:39:54.339Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#---------------------------- S T A R T - E V A L U A T I O N - P L O T S ----------------------------\n",
    "\n",
    "#---------------------------- H E A T M A P S ----------------------------\n",
    "start, stop = 0, 5\n",
    "\n",
    "dataset_names = [dataset[0] for dataset_index,dataset in enumerate(test_datasets) if start <= dataset_index <= stop]\n",
    "plot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, f\"{evaluations_folder}dataset[{start}-{stop}]-plot_heatmap_auc_datasets_configs.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- A V E R A G E - W EI G H T - H I S T O G R A M -------------------------\n",
    "transformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \n",
    "plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- S C A T T E R -------------------------\n",
    "plot_scatter_train_auc_test_auc_best_performer_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_best_performer_transformations.png\")\n",
    "plot_scatter_train_auc_test_auc_weighted_average_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_weighted_average_transformations.png\")\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- T R A N S F O R M A T I O N S - H I S T O G R A M  ----------------------------\n",
    "transformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \n",
    "plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- I N S T A N C E S - A U C - H I S T O G R A M  --------------------------------\n",
    "plot_histogram_average_test_AUC_instance(ensemble_results, f\"{evaluations_folder}plot_histogram_average_test_AUC_instance.png\")\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- M O D E L - A U C - H I S T O G R A M  ----------------------------------------\n",
    "plot_histogram_average_train_auc_model(ensemble_results, f\"{evaluations_folder}plot_histogram_average_train_auc_model.png\" )\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- T I M E S - M O D E L - H I S T O G R A M  ------------------------------------\n",
    "plot_histogram_times_picked_model(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_model.png\" )\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#------------------ T I M E S - T R A N S F O R M A T I O N - H I S T O G R A M  ------------------\n",
    "plot_histogram_times_picked_transformation(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_transformation.png\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------- S T A T I S T I C A L - T E S T S -------------------------\n",
    "ttest_auc_performance_compared_to_baseline(dictionary=ensemble_results,alpha=0.05)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
