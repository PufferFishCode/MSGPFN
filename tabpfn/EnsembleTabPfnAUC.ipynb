{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c02861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:32.554174Z",
     "start_time": "2023-07-22T10:31:29.299974Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD, FastICA\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, RobustScaler,MaxAbsScaler\n",
    "from scipy.stats import ttest_rel\n",
    "import copy\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from scripts import tabular_metrics\n",
    "from scripts.differentiable_pfn_evaluation import eval_model, eval_model_range\n",
    "from scripts.model_builder import (\n",
    "    get_model,\n",
    "    get_default_spec,\n",
    "    save_model,\n",
    "    load_model,\n",
    "    load_model_only_inference,\n",
    ")\n",
    "from scripts.transformer_prediction_interface import (\n",
    "    transformer_predict,\n",
    "    get_params_from_config,\n",
    "    TabPFNClassifier,\n",
    ")\n",
    "\n",
    "from datasets import (\n",
    "    load_openml_list,\n",
    "    open_cc_dids,\n",
    "    open_cc_valid_dids,\n",
    "    test_dids_classification,\n",
    ")\n",
    "\n",
    "base_path = '.'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set the seed for the random number generator provided by PyTorch\n",
    "torch.manual_seed(0)\n",
    "# Set the seed for the random number generator provided by NumPy\n",
    "np.random.seed(0)\n",
    "# Set the seed for the random number generator provided by Python\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6df194",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb029b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:38.271068Z",
     "start_time": "2023-07-22T10:31:38.260847Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_datasets(selector, task_type, suite='cc'):\n",
    "    if task_type == 'binary':\n",
    "        ds = valid_datasets_binary if selector == 'valid' else test_datasets_binary\n",
    "    else:\n",
    "        if suite == 'openml':\n",
    "            ds = valid_datasets_multiclass if selector == 'valid' else test_datasets_multiclass\n",
    "        elif suite == 'cc':\n",
    "            ds = cc_valid_datasets_multiclass if selector == 'valid' else cc_test_datasets_multiclass\n",
    "        else:\n",
    "            raise Exception(\"Unknown suite\")\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d538e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:38.736541Z",
     "start_time": "2023-07-22T10:31:38.717096Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_for_evaluation(dataset_index, test_datasets):\n",
    "    dataset = test_datasets[dataset_index]\n",
    "    dataset_name = dataset[0]\n",
    "    dataset_size = dataset[1].shape\n",
    "\n",
    "    print(f'Evaluation dataset name: {dataset_name} size {dataset_size} --- {dataset_index}/{len(test_datasets)}')\n",
    "\n",
    "    xs, ys = dataset[1].clone(), dataset[2].clone()\n",
    "    \n",
    "    # Replace NaN values with 0\n",
    "    xs[torch.isnan(xs)] = 0\n",
    "    xs[torch.isinf(xs)] = 0\n",
    "    \n",
    "    ys[torch.isnan(ys)] = 0\n",
    "    ys[torch.isinf(ys)] = 0\n",
    "    \n",
    "    eval_position = xs.shape[0] // 2\n",
    "    train_xs, train_ys = xs[0:eval_position], ys[0:eval_position]\n",
    "    test_xs, test_ys = xs[eval_position:], ys[eval_position:]\n",
    "    return train_xs, train_ys, test_xs, test_ys, dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a05251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.436676Z",
     "start_time": "2023-07-22T10:31:39.788499Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_samples = 10000\n",
    "bptt = 10000\n",
    "\n",
    "cc_test_datasets_multiclass, cc_test_datasets_multiclass_df = load_openml_list(open_cc_dids, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = max_samples, num_feats=100, return_capped=True)\n",
    "cc_valid_datasets_multiclass, cc_valid_datasets_multiclass_df = load_openml_list(open_cc_valid_dids, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = max_samples, num_feats=100, return_capped=True)\n",
    "\n",
    "# Loading longer OpenML Datasets for generalization experiments (optional)\n",
    "# test_datasets_multiclass, test_datasets_multiclass_df = load_openml_list(test_dids_classification, multiclass=True, shuffled=True, filter_for_nan=False, max_samples = 10000, num_feats=100, return_capped=True)\n",
    "\n",
    "random.shuffle(cc_valid_datasets_multiclass)\n",
    "\n",
    "\n",
    "model_string, longer, task_type = '', 1, 'multiclass'\n",
    "eval_positions = [1000]\n",
    "bptt = 2000\n",
    "    \n",
    "test_datasets, valid_datasets = get_datasets('test', task_type, suite='cc'), get_datasets('valid', task_type, suite='cc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01759e3",
   "metadata": {},
   "source": [
    "## Plotting Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04dd6ec",
   "metadata": {},
   "source": [
    "### Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a86427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.467785Z",
     "start_time": "2023-07-22T10:31:52.442280Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_dict_to_pickle_file(dictionary, filename, keys_to_exclude):\n",
    "    dict_to_plot = clean_result_dictionary_for_evaluation(dictionary, keys_to_exclude) \n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(dict_to_plot, file)\n",
    "\n",
    "def clean_result_dictionary_for_evaluation(dictionary, keys_to_exclude):\n",
    "    def clean_result_dict(d, keys):\n",
    "        new_d = {}\n",
    "        for key, value in d.items():\n",
    "            if key not in keys:\n",
    "                if isinstance(value, dict):\n",
    "                    new_d[key] = clean_result_dict(value, keys)  # Pass the 'keys' parameter instead of 'keys_to_exclude'\n",
    "                else:\n",
    "                    new_d[key] = copy.deepcopy(value)\n",
    "        return new_d\n",
    "\n",
    "    new_dict = clean_result_dict(dictionary, keys_to_exclude)\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ed4d5",
   "metadata": {},
   "source": [
    "### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794876a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.483882Z",
     "start_time": "2023-07-22T10:31:52.471300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, result_path):\n",
    "    \"\"\"\n",
    "    Generate a heatmap to visualize the test data AUC scores of ensemble configurations across specified datasets.\n",
    "\n",
    "    Parameters:\n",
    "        ensemble_results (dict): A dictionary containing AUC scores for ensemble configurations and datasets.\n",
    "        dataset_names (list): A list of dataset names to be included in the heatmap.\n",
    "        result_path (str): The file path to save the resulting heatmap.\n",
    "\n",
    "    Returns:\n",
    "        None: The function displays the heatmap and saves it to the specified file path.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    temp_dic_map =  {'mfeat-fourier':\"fourier\",\n",
    "                     'mfeat-morphological':\"morph\",\n",
    "                     'credit-approval':\"credit-app\",\n",
    "                     'diabetes':\"diabetes\", \n",
    "                     'tic-tac-toe':\"tic-tac-toe\",\n",
    "                     'vehicle':\"vehicle\", \n",
    "                     'analcatdata_dmft':\"bankruptcy\",\n",
    "                     'pc4':'pc4',\n",
    "                     'pc3': 'pc3',\n",
    "                     \"kc2\":\"kc2\",\n",
    "                     \"pc1\":\"pc1\",\n",
    "                     \"ilpd\":\"ilpd\",\n",
    "                     'climate-model-simulation-crashes':\"climate\",\n",
    "                    \"mfeat-karhunen\": \"karhunen\",\n",
    "                    \"cmc\": \"cmc\",\n",
    "                    \"credit-g\":\"credit-g\",\n",
    "                     \"eucalyptus\":\"eucal.\",\n",
    "                     \"mfeat-zernike\":\"zernike\",\n",
    "                     \"blood-transfusion-service-center\":\"blood trans.\",\n",
    "                     \"cylinder-bands\":\"cyl. bands\",\n",
    "                     \"steel-plates-fault\":\"st. plates\"}\n",
    "    # Create lists to store data for the heatmap\n",
    "    datasets = []\n",
    "    ensemble_names = []\n",
    "    auc_scores = []\n",
    "    # Extract data from the ensemble_results dictionary\n",
    "    for ensemble_name, ensemble_data in ensemble_results.items():\n",
    "    \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            if dataset_name in dataset_names:\n",
    "                \n",
    "                #dataset_name = dataset_name.replace(\"mfeat-\", \"\")[:7] if dataset_name.replace(\"mfeat-\", \"\")[:8][-1] == \"-\" else dataset_name.replace(\"mfeat-\", \"\")[:8]\n",
    "                \n",
    "                datasets.append(temp_dic_map[dataset_name])\n",
    "                \n",
    "                ensemble_name = ensemble_name.replace(\"best_performer\", \"bp\").replace(\"weighted_average\", \"wa\")\n",
    "                ensemble_names.append(ensemble_name)\n",
    "                \n",
    "                auc_scores.append(ensemble_dict['auc'])\n",
    "\n",
    "    \n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        'Ensemble Configuration': ensemble_names,\n",
    "        'Dataset': datasets,\n",
    "        'AUC Score': auc_scores\n",
    "    })\n",
    "\n",
    "    # Reshape the DataFrame to have \"Dataset\" as rows, \"Ensemble Configuration\" as columns, and \"AUC Score\" as values\n",
    "    df_pivot = df.pivot(index='Ensemble Configuration', columns='Dataset', values='AUC Score')\n",
    "\n",
    "    # Create the heatmap using seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns_plot = sns.heatmap(df_pivot, annot=True, cmap='flare', fmt=\".2f\",)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # plt.title(\"Test AUC of Ensemble Configurations Across All Datasets\")\n",
    "    plt.xlabel(\"Datasets\", fontsize=18)\n",
    "    plt.ylabel(\"Ensemble Configuration\", fontsize=18)\n",
    "\n",
    "    plt.setp(sns_plot.get_xticklabels(), rotation=30, ha='right')\n",
    "    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d92b31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.514967Z",
     "start_time": "2023-07-22T10:31:52.489562Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_average_weights_data_transformations(dictionary, transformation_names, result_path):\n",
    "    transformation_weight_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "       \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            if \"data_ens_dict\" not in ensemble_dict:\n",
    "                continue\n",
    "                \n",
    "            for model_transformation_key, model_transformation in ensemble_dict[\"data_ens_dict\"].items():\n",
    "            \n",
    "                transformation_name = model_transformation_key.split(\"-\")[-1]\n",
    "                \n",
    "                if transformation_name not in transformation_weight_dict:\n",
    "                    transformation_weight_dict[transformation_name] = []\n",
    "                    \n",
    "                transformation_weight_dict[transformation_name].append(model_transformation[\"data_weight\"])\n",
    "                \n",
    "    for transformation_name, data_weight_list in transformation_weight_dict.items():\n",
    "        transformation_weight_dict[transformation_name] = sum(transformation_weight_dict[transformation_name]) / len(transformation_weight_dict[transformation_name])\n",
    "\n",
    "    # Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    transformation_names = list(transformation_weight_dict.keys())\n",
    "    average_weights = list(transformation_weight_dict.values())\n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns_plot = sns.barplot(x=transformation_names, y=average_weights)\n",
    "    plt.xlabel(\"Transformations\")\n",
    "    plt.ylabel(\"Average Weight\")\n",
    "    plt.title(\"Average Weight of Transformations Across Ensembles and All Datasets\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "    # Annotate each bar with its corresponding value\n",
    "    for index, value in enumerate(average_weights):\n",
    "        plt.text(index, value, f\"{value:.2f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26549aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.546123Z",
     "start_time": "2023-07-22T10:31:52.518805Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_scatter_train_auc_test_auc_best_performer_transformations(dictionary, result_path):\n",
    "    transformation_best_performer_aucs = {\"test_auc\": [],\n",
    "                                          \"train_auc\": [],\n",
    "                                          \"split_type\":[]}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if \"best_performer\" not in ensemble_name: \n",
    "            continue\n",
    "        split_type = ensemble_name.split(\"/\")[-1]\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            test_auc = ensemble_dict[\"auc\"]\n",
    "            train_auc = -1\n",
    "            \n",
    "            for model_transformation_key, model_transformation in ensemble_dict[\"data_ens_dict\"].items():\n",
    "                if model_transformation[\"data_weight\"] == 1:\n",
    "                    train_auc = model_transformation[\"data_auc\"]\n",
    "                    break\n",
    "                    \n",
    "            transformation_best_performer_aucs[\"test_auc\"].append(test_auc)\n",
    "            transformation_best_performer_aucs[\"train_auc\"].append(train_auc)\n",
    "            transformation_best_performer_aucs[\"split_type\"].append(split_type)\n",
    "              \n",
    "    data = pd.DataFrame(transformation_best_performer_aucs)\n",
    "\n",
    "    # Create the scatterplot with hue\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns_plot = sns.scatterplot(x=\"train_auc\", y=\"test_auc\", hue=\"split_type\", data=data)\n",
    "\n",
    "    # Fit the linear regression line\n",
    "    sns_plot = sns.regplot(x=\"train_auc\", y=\"test_auc\", scatter=False, color=\"gray\", data=data)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel(\"Train AUC\")\n",
    "    plt.ylabel(\"Test AUC\")\n",
    "    plt.title(\"Predicted AUC based on train data compared to test AUC on test data of best performer transformation\")\n",
    "\n",
    "    plt.legend(title=\"Type of Train Split\")\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight', dpi=500)\n",
    "    plt.show()\n",
    "    \n",
    "    ## ------------------------------ C O R R E L A T I O N ------------------------------\n",
    "    correlation = data['test_auc'].corr(data['train_auc'])\n",
    "    print(\"Best performer: Correlation between test_auc and train_auc:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a08fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.578175Z",
     "start_time": "2023-07-22T10:31:52.549215Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_scatter_train_auc_test_auc_weighted_average_transformations(dictionary, result_path):\n",
    "    transformation_best_performer_aucs = {\"test_auc\": [],\n",
    "                                          \"train_auc\": [],\n",
    "                                          \"split_type\":[]}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if \"weighted_average\" not in ensemble_name: \n",
    "            continue\n",
    "        split_type = ensemble_name.split(\"/\")[-1]\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            test_auc = ensemble_dict[\"auc\"]\n",
    "            train_auc = 0\n",
    "            \n",
    "            if \"data_ens_dict\" not in ensemble_dict: \n",
    "                continue\n",
    "            for model_transformation_key, model_transformation in ensemble_dict[\"data_ens_dict\"].items():\n",
    "                train_auc += model_transformation[\"data_weight\"] * model_transformation[\"data_auc\"]\n",
    "                    \n",
    "            transformation_best_performer_aucs[\"test_auc\"].append(test_auc)\n",
    "            transformation_best_performer_aucs[\"train_auc\"].append(train_auc)\n",
    "            transformation_best_performer_aucs[\"split_type\"].append(split_type)\n",
    "              \n",
    "    data = pd.DataFrame(transformation_best_performer_aucs)\n",
    "\n",
    "    # Create the scatterplot with hue\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns_plot = sns.scatterplot(x=\"train_auc\", y=\"test_auc\", hue=\"split_type\", data=data)\n",
    "\n",
    "    # Fit the linear regression line\n",
    "    sns_plot = sns.regplot(x=\"train_auc\", y=\"test_auc\", scatter=False, color=\"gray\", data=data)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel(\"Train AUC\")\n",
    "    plt.ylabel(\"Test AUC\")\n",
    "    plt.title(\"Predicted AUC based on train data compared to test AUC on test data of weighted average transformation\")\n",
    "\n",
    "    plt.legend(title=\"Type of Train Split\")\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight', dpi=500)\n",
    "    plt.show()\n",
    "    \n",
    "    ## ------------------------------ C O R R E L A T I O N ------------------------------\n",
    "    correlation = data['test_auc'].corr(data['train_auc'])\n",
    "    print(\"Weighted average: Correlation between test_auc and train_auc:\", correlation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15627092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_train_auc_test_auc_weighted_average_combined(dictionary, result_path):\n",
    "    transformation_best_performer_aucs = {\"test_auc\": [],\n",
    "                                          \"train_auc\": []}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if ensemble_name == \"baseline\": \n",
    "            continue\n",
    "            \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            test_auc = ensemble_dict[\"auc\"]\n",
    "            \n",
    "            train_auc = 0\n",
    "            for model_name, model_content in ensemble_dict[\"model_dict\"].items():\n",
    "                train_auc += model_content[\"model_auc\"] * model_content[\"model_weight\"] \n",
    "                    \n",
    "            transformation_best_performer_aucs[\"test_auc\"].append(test_auc)\n",
    "            transformation_best_performer_aucs[\"train_auc\"].append(train_auc) \n",
    "              \n",
    "    data = pd.DataFrame(transformation_best_performer_aucs)\n",
    "\n",
    "    # Create the scatterplot with hue\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns_plot = sns.scatterplot(x=\"train_auc\", y=\"test_auc\", data=data)\n",
    "\n",
    "    # Fit the linear regression line\n",
    "    sns_plot = sns.regplot(x=\"train_auc\", y=\"test_auc\", scatter=False, color=\"gray\", data=data)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel(\"Train AUC\")\n",
    "    plt.ylabel(\"Test AUC\")\n",
    "    plt.title(\"Predicted AUC based on train data compared to test AUC on test data of weighted average transformation\")\n",
    "\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight', dpi=500)\n",
    "    plt.show()\n",
    "    \n",
    "    ## ------------------------------ C O R R E L A T I O N ------------------------------\n",
    "    correlation = data['test_auc'].corr(data['train_auc'])\n",
    "    print(\"Weighted average: Correlation between test_auc and train_auc:\", correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506727df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.609440Z",
     "start_time": "2023-07-22T10:31:52.582172Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_average_weights_data_transformations(dictionary, transformation_names, result_path):\n",
    "    transformation_weight_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "       \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            if \"data_ens_dict\" not in ensemble_dict:\n",
    "                continue\n",
    "                \n",
    "            for model_transformation_key, model_transformation in ensemble_dict[\"data_ens_dict\"].items():\n",
    "            \n",
    "                transformation_name = model_transformation_key.split(\"-\")[-1]\n",
    "                \n",
    "                if transformation_name not in transformation_weight_dict:\n",
    "                    transformation_weight_dict[transformation_name] = []\n",
    "                    \n",
    "                transformation_weight_dict[transformation_name].append(model_transformation[\"data_weight\"])\n",
    "                \n",
    "    for transformation_name, data_weight_list in transformation_weight_dict.items():\n",
    "        transformation_weight_dict[transformation_name] = sum(transformation_weight_dict[transformation_name]) / len(transformation_weight_dict[transformation_name])\n",
    "\n",
    "    # Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    transformation_names = list(transformation_weight_dict.keys())\n",
    "    average_weights = list(transformation_weight_dict.values())\n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns_plot = sns.barplot(x=transformation_names, y=average_weights)\n",
    "    plt.xlabel(\"Transformations\")\n",
    "    plt.ylabel(\"Average Weight\")\n",
    "    plt.title(\"Average Weight of Transformations Across Ensembles and All Datasets\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "    # Annotate each bar with its corresponding value\n",
    "    for index, value in enumerate(average_weights):\n",
    "        plt.text(index, value, f\"{value:.2f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd6e8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.641305Z",
     "start_time": "2023-07-22T10:31:52.613098Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_average_train_auc_model(dictionary, result_path):\n",
    "    model_auc_dict = {}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if \"baseline\" == ensemble_name: \n",
    "            continue\n",
    "        \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            for model_name, model_content in ensemble_dict[\"model_dict\"].items():\n",
    "                \n",
    "                model_auc = model_content[\"model_auc\"]\n",
    "                if model_name not in model_auc_dict:\n",
    "                    model_auc_dict[model_name] = []\n",
    "                \n",
    "                model_auc_dict[model_name].append(model_auc)\n",
    "                \n",
    "    # for model_name, model_auc_list in model_auc_dict.items():\n",
    "    #     model_auc_dict[model_name] = sum(model_auc_list) / len(model_auc_list)\n",
    "                \n",
    "    # Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    \n",
    "    data = pd.DataFrame(model_auc_dict)\n",
    "    #print(data)\n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Step 1: Melt the DataFrame to transform it into a long-form\n",
    "    melted_data = data.melt()\n",
    "    # Step 2: Calculate the mean and 95% confidence intervals for each column\n",
    "    mean_data = melted_data.groupby('variable')['value'].mean().reset_index()\n",
    "    ci_data = melted_data.groupby('variable')['value'].agg(lambda x: x.sem() * 1.96).reset_index()\n",
    "    mean_data['ci'] = ci_data['value']\n",
    "\n",
    "    sns_plot = sns.barplot(x='variable', y='value', data=mean_data)\n",
    "\n",
    "    # Step 4: Add error bars with 95% confidence intervals\n",
    "    plt.errorbar(x=mean_data['variable'], y=mean_data['value'], yerr=mean_data['ci'], fmt='none', color='black', capsize=4)\n",
    "\n",
    "    # Step 5: Annotate each bar with its value\n",
    "    for index, row in mean_data.iterrows():\n",
    "        plt.text(index, row['value'] + row['ci'], f\"{row['value']:.4f}\", ha='center', va='bottom')\n",
    "\n",
    "    \n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Average AUC\")\n",
    "    plt.title(\"Average AUC of Models Across Ensembles and All Datasets\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024a859",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.672581Z",
     "start_time": "2023-07-22T10:31:52.644893Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_histogram_average_train_AUC_instance(dictionary, result_path):\n",
    "    \"\"\"\n",
    "    Generate a bar plot to visualize the average train AUC scores across data transformations.\n",
    "\n",
    "    Parameters:\n",
    "        dictionary (dict): A dictionary containing AUC scores for different data transformations.\n",
    "        result_path (str): The file path to save the resulting bar plot.\n",
    "\n",
    "    Returns:\n",
    "        None: The function displays the bar plot and saves it to the specified file path.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    name_dict = {\"FastICA\": \"ICA\",\n",
    "                 \"FeatureAgglomeration\": \"Feat Aggl.\",\n",
    "                 \"identity\": \"Identity\",\n",
    "                 \"KernelPCA\":\"KernelPCA\",\n",
    "                 \"PCA\":\"PCA\",\n",
    "                 \"PowerTransformer\":\"Pow. Trans.\",\n",
    "                 \"QuantileTransformer\": \"Quant. Trans.\",\n",
    "                 \"RobustScaler\": \"Rob. Scaler\", \n",
    "                 \"TruncatedSVD\":\"Trunc. SVD\"\n",
    "    }\n",
    "    # Idea: Data auc is strongly correlated with the test auc\n",
    "    aucs_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if ensemble_name == \"baseline\": \n",
    "            continue\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items(): \n",
    "                \n",
    "            for model_name_transformation, transformation_content in ensemble_dict[\"data_ens_dict\"].items():\n",
    "                transformation_name = model_name_transformation.split(\"-\")[-1]\n",
    "                transformation_name = name_dict[transformation_name]\n",
    "                if transformation_name not in aucs_dict: \n",
    "                    aucs_dict[transformation_name]= []\n",
    "                aucs_dict[transformation_name].append(transformation_content[\"data_auc\"])\n",
    "                \n",
    "\n",
    "   \n",
    "      \n",
    "    data = pd.DataFrame(aucs_dict)\n",
    "        \n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Step 1: Melt the DataFrame to transform it into a long-form\n",
    "    melted_data = data.melt()\n",
    "    \n",
    "    # Step 2: Calculate the mean and 95% confidence intervals for each column\n",
    "    mean_data = melted_data.groupby('variable')['value'].mean().reset_index()\n",
    "\n",
    "    \n",
    "        \n",
    "    ci_data = melted_data.groupby('variable')['value'].agg(lambda x: x.sem() * 1.96).reset_index()\n",
    "\n",
    "    mean_data['ci'] = ci_data['value']\n",
    "    \n",
    "    mean_data = mean_data.sort_values(by=\"value\", ascending=False).reset_index( drop=True)\n",
    "\n",
    "\n",
    "    sns_plot = sns.barplot(x='variable',\n",
    "                           y='value', \n",
    "                           data=mean_data, \n",
    "                           palette='flare_r',\n",
    "                           color='value')\n",
    "\n",
    "    # Step 4: Add error bars with 95% confidence intervals\n",
    "    plt.errorbar(x=mean_data['variable'], \n",
    "                 y=mean_data['value'],\n",
    "                 yerr=mean_data['ci'], \n",
    "                 fmt='none', \n",
    "                 color='black',\n",
    "                 capsize=4)\n",
    "\n",
    "    # Step 5: Annotate each bar with its value\n",
    "    for index, row in mean_data.iterrows():\n",
    "        plt.text(index, row['value'] + row['ci'], f\"{row['value']:.4f}\", ha='left', rotation=35, va='bottom')\n",
    "\n",
    "    \n",
    "    # Set labels and title\n",
    "    plt.xlabel(\"\", fontsize=18)\n",
    "    plt.ylabel(\"Average Train AUC\", fontsize=18)\n",
    "    \n",
    "    plt.ylim(0.5,1)\n",
    "    #plt.title(\"Average AUC of Train Data Across Data Transformations\", fontsize=16)\n",
    "\n",
    "    plt.xticks(rotation=35, ha=\"right\")  # Rotate x-axis labels for better readability\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eee8af2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:52.750806Z",
     "start_time": "2023-07-22T10:31:52.731232Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_times_picked_model(dictionary, result_path):\n",
    "    \"\"\"Plots a histogram to visualize the number of times each model is picked as the 'Best Performer'.\n",
    "\n",
    "    The function takes a dictionary containing model ensemble data and generates a histogram plot\n",
    "    to display the frequency of each model being selected as the best performer.\n",
    "    \n",
    "    Parameters:\n",
    "        dictionary (dict): A dictionary containing the model ensemble data.\n",
    "            The keys of the dictionary represent ensemble names, and the values are dictionaries\n",
    "            containing data for each ensemble.\n",
    "            \n",
    "        result_path (str): The file path to save the resulting plot. \n",
    "\n",
    "    Returns:\n",
    "        None: The function only displays the histogram plot and does not return any value.\n",
    "    \"\"\"\n",
    "    models_times_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if \"baseline\" == ensemble_name or \"best_performer\" not in ensemble_name:\n",
    "            continue\n",
    "            \n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            for model_name, model_content in ensemble_dict[\"model_dict\"].items():\n",
    "                # optional: replace model_ with PFN                \n",
    "                model_name = model_name.replace(\"model_\", \"\")\n",
    "                if model_content[\"model_weight\"] == 1: \n",
    "                    if model_name not in models_times_dict: \n",
    "\n",
    "                        \n",
    "                        models_times_dict[model_name] = 0\n",
    "                    models_times_dict[model_name] +=1\n",
    "                    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame(list(models_times_dict.items()), columns=['Model', 'Value'])\n",
    " \n",
    "    df = df.sort_values(by='Value', ascending=False)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    # Create the seaborn histogram\n",
    "    sns_plot = sns.barplot(x='Model', y='Value', data=df, palette=sns.color_palette('flare_r', n_colors=len(df)))\n",
    "\n",
    "    plt.xlabel(\"PFN-Instances\")\n",
    "    plt.ylabel(\"Number of Occurrences\")\n",
    "    # plt.title(\"Frequency of Selecting the PFN-Instances as Best Performer\") Best Performer Frequency - PFN Instances \n",
    "    plt.xticks(rotation=35, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    # Color x-ticks containing specific substrings differently\n",
    "    for tick_label in sns_plot.get_xticklabels():\n",
    "        if \"bag\" in tick_label.get_text():\n",
    "            tick_label.set_color((122/ 255, 35/ 255, 35/ 255)) \n",
    "        elif \"gp_\" in tick_label.get_text():\n",
    "            tick_label.set_color((21/ 255, 22/ 255, 92/ 255)) \n",
    "        elif \"causal_\" in tick_label.get_text():\n",
    "            tick_label.set_color((166/ 255, 106/ 255, 41/ 255)) \n",
    "    #-------------------------------------------------------------\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for index, value in enumerate(df['Value']):\n",
    "        sns_plot.text(index, value, str(value), ha='center', va='bottom')\n",
    "    \n",
    "        \n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6172ef69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:53.841603Z",
     "start_time": "2023-07-22T10:31:53.828551Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histogram_times_picked_transformation(dictionary, result_path):\n",
    "    transformation_times_dict = {}\n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            if \"data_ens_dict\" not in ensemble_dict:\n",
    "                continue\n",
    "\n",
    "            for model_name_key, model_data in ensemble_dict[\"data_ens_dict\"].items():\n",
    "                transformation_key = model_name_key.split(\"-\")[-1]\n",
    "\n",
    "                if transformation_key not in transformation_times_dict:\n",
    "                    transformation_times_dict[transformation_key] = 0\n",
    "                    \n",
    "                if model_data[\"data_weight\"] == 1:\n",
    "                    transformation_times_dict[transformation_key] += 1\n",
    "\n",
    "    # Convert the dictionary into two separate lists (x-axis and y-axis)\n",
    "    tranformations_names = list(transformation_times_dict.keys())\n",
    "    times = list(transformation_times_dict.values())\n",
    "\n",
    "    # Create the histogram using Seaborn's barplot\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns_plot = sns.barplot(x=tranformations_names, y=times)\n",
    "    plt.xlabel(\"Transformation\")\n",
    "    plt.ylabel(\"Times\")\n",
    "    plt.title(\"Times a Model is the best performing one\")\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "    \n",
    "    # Annotate each bar with its corresponding value\n",
    "    for index, value in enumerate(times):\n",
    "        plt.text(index, value, f\"{value:.2f}\", ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of axis labels    \n",
    "    sns_plot.figure.set_dpi(500)\n",
    "    \n",
    "    plt.show()\n",
    "    sns_plot.figure.savefig(result_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c4ee80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:31:54.466648Z",
     "start_time": "2023-07-22T10:31:54.449820Z"
    }
   },
   "outputs": [],
   "source": [
    "def ttest_auc_performance_compared_to_baseline(dictionary,  alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs paired t-test to compare AUC performances of algorithms to a baseline.\n",
    "\n",
    "    The function takes a dictionary containing AUC (Area Under the Curve) values for different\n",
    "    algorithms' performances on multiple datasets. It calculates the paired t-test for each\n",
    "    algorithm's performance against the baseline's AUC values and determines if the performance\n",
    "    difference is statistically significant based on the given significance level (alpha).\n",
    "\n",
    "    Parameters:\n",
    "        dictionary (dict): A dictionary containing AUC values for different algorithms on multiple datasets.\n",
    "            The keys of the dictionary represent the algorithm names, and the values are nested dictionaries\n",
    "            containing the AUC values for each algorithm on various datasets.\n",
    "            \n",
    "        alpha (float, optional): The significance level (threshold) for the t-test.\n",
    "            Default is 0.05.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the statistical significance of AUC performance\n",
    "            compared to the baseline for each algorithm. The dictionary is structured as follows:\n",
    "            \n",
    "            {\n",
    "                'algorithm_1': {'significant': is_significant_1},\n",
    "                'algorithm_2': {'significant': is_significant_2},\n",
    "                ...\n",
    "            }\n",
    "            \n",
    "            where 'is_significant_i' (True/False) indicates whether the AUC performance of the 'algorithm_i'\n",
    "            is significantly different from the baseline's AUC value at the given significance level.\n",
    "                       \n",
    "    Note:\n",
    "        The function uses the paired t-test (ttest_rel) from the scipy.stats library to calculate the\n",
    "        t-statistic and p-value for comparing the AUC performances. A two-tailed test is performed to\n",
    "        check for differences in both directions.\n",
    "    \"\"\"\n",
    "    \n",
    "    auc_dictionary = {}\n",
    "    \n",
    "    for ensemble_name, ensemble_data in dictionary.items():\n",
    "        if ensemble_name not in auc_dictionary: \n",
    "            auc_dictionary[ensemble_name] = []\n",
    "        for dataset_name, ensemble_dict in ensemble_data.items():\n",
    "            auc_dictionary[ensemble_name].append(ensemble_dict[\"auc\"]) \n",
    "\n",
    "    \n",
    "    for ensemble_name, ensemble_auc in auc_dictionary.items():\n",
    "        if \"baseline\" == ensemble_name:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        baseline_auc_values = np.array(auc_dictionary[\"baseline\"])\n",
    "        algorithm_auc_values = np.array(auc_dictionary[ensemble_name])\n",
    "\n",
    "        if len(baseline_auc_values) != len(algorithm_auc_values):\n",
    "            raise ValueError(\"Number of AUC values for the baseline and algorithm must be the same.\")\n",
    "\n",
    "        # Compute the differences between the two sets of AUC values\n",
    "        differences = algorithm_auc_values - baseline_auc_values\n",
    "\n",
    "        # Perform paired t-test\n",
    "        t_statistic, p_value = ttest_rel(algorithm_auc_values, baseline_auc_values)\n",
    "\n",
    "        # Check if the p-value is less than the significance level\n",
    "        is_significant = p_value < alpha\n",
    "\n",
    "        auc_dictionary[ensemble_name]= {\"significant\": is_significant}\n",
    "        \n",
    "    auc_dictionary.pop(\"baseline\")\n",
    "    # Create a DataFrame from the dictionary\n",
    "    \n",
    "    return auc_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4844da5e",
   "metadata": {},
   "source": [
    "# PFN Ensemble Class : Ensemble over TabPFNs with Different Data Transformations\n",
    "\"\"\"\" we introduce the PFNENsemble technique....\n",
    "\n",
    "The `PFNEnsemble` class is designed to perform ensembling over TabPFNs and/or different data transformations. The main process path involves a two-step procedure: first, fitting the PFNs to the training data, and second, using the trained PFN instances with the respective data transformation to predict probabilities on the test data.\n",
    "\n",
    "### Initialization\n",
    "- `model_storage_folders`: A list of folders containing the pre-trained TabPFN instances.\n",
    "- `device`: The device on which the PFN instances are loaded and trained (e.g., \"cpu\" or \"gpu\").\n",
    "- `N_ensemble_configurations`: The number of ensemble configurations to use during training.\n",
    "- `verbose`: A flag indicating whether to display verbose output during the ensemble process.\n",
    "\n",
    "### Methods\n",
    "\n",
    "1. `fit(train_xs, train_ys)`: Fits the PFN instances to the training data. As the TabPFN gets inputs, as well as labels of the training data we can estimate the performance of the test data based on the train data. \n",
    "   - `train_xs`: The input features of the training data.\n",
    "   - `train_ys`: The target labels of the training data.\n",
    "\n",
    "2. `predict_proba(test_xs, data_ensemble_config=None, model_ensemble_config=None, data_preprocess_config=None, multiple_models=False, pre_processing=False)`: Predicts probabilities on the test data using the ensembled PFN instances.\n",
    "   - `test_xs`: The input features of the test data.\n",
    "   - `data_ensemble_config`: A dictionary specifying the data-level ensembling configuration.\n",
    "   - `model_ensemble_config`: A dictionary specifying the PFN instance-level ensembling configuration.\n",
    "   - `data_preprocess_config`: A dictionary specifying the data preprocessing configuration.\n",
    "   - `multiple_models`: A flag indicating whether to use multiple PFN instances for ensembling.\n",
    "   - `pre_processing`: A flag indicating whether to perform data preprocessing before ensembling.\n",
    "\n",
    "### Data Ensembling and PFN Ensembling\n",
    "The class supports the following ensembling configurations:\n",
    "\n",
    "- **Data Ensembling**: Data Ensembling uses data transformations, which can be applied to the data and after the forward pass aggregated with different methods. The available preprocessing methods are specified in the `data_preprocess_config` dictionary. \n",
    "\n",
    "- **PFN Ensembling**: PFN ensembling can be performed using PFN instances, whereby their individual results can be aggregated using different techniques.\n",
    "\n",
    "Data ensembling can be combined with PFN ensembling for a more comprehensive approach.\n",
    "\n",
    "### Weight Computation\n",
    "As PFNs have the advantage to have access to the training inputs and labels we can leverage that to compute a metric which gives us a prediction of how well a PFN-Instance / data transformation will perform on the test data. We therefore compute weights based on the expected prediction performance for each used PFN / transformation.\n",
    "The two weight types are supported: \"weighted_average\" and \"best_performer\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e3d0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:38:47.165914Z",
     "start_time": "2023-07-22T10:38:47.066774Z"
    }
   },
   "outputs": [],
   "source": [
    "class PFNEnsemble:\n",
    "    def __init__(self, \n",
    "                 model_storage_folders=[\"models_gp\"],\n",
    "                 device=\"cpu\",\n",
    "                 N_ensemble_configurations=5,\n",
    "                 verbose=False):\n",
    "        # Maps model_index to loaded model\n",
    "        self.model_dict = {}\n",
    "        self.data_ens_dict = {}\n",
    "        \n",
    "        for model_storage_folder in model_storage_folders:\n",
    "            # Iterate over all files in the model_path directory\n",
    "            for model_index, model_string in enumerate(os.listdir(model_storage_folder)):\n",
    "                if model_string.endswith('.cpkt'):  # Assuming the models have a .cpkt extension\n",
    "                    loaded_model = TabPFNClassifier(base_path=model_storage_folder,\n",
    "                                                    model_string=model_string[:-5],\n",
    "                                                    N_ensemble_configurations=N_ensemble_configurations,\n",
    "                                                    batch_size_inference=1, \n",
    "                                                    no_preprocess_mode=True)\n",
    "                    \n",
    "        \n",
    "                    self.model_dict[model_string.split('.')[0]] = {\"model\": loaded_model}\n",
    "\n",
    "    def fit(self, train_xs, train_ys):\n",
    "        for model_name, model_content in self.model_dict.items():\n",
    "            model = model_content[\"model\"]\n",
    "            model.fit(train_xs, train_ys)\n",
    "            \n",
    "    def get_split_of_train_data_simple(self, train_x, train_y, split_share):\n",
    "        temp_split_pos = int(train_x.shape[0] * split_share)\n",
    "        \n",
    "        temp_split_train_x = train_x[:temp_split_pos]\n",
    "        temp_split_test_x = train_x[temp_split_pos:]\n",
    "        \n",
    "        temp_split_train_y = train_y[:temp_split_pos]\n",
    "        temp_split_test_y = train_y[temp_split_pos:]\n",
    "        return [(temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y)]\n",
    "    \n",
    "                \n",
    "    def get_split_of_train_data_bagging(self, train_x, train_y, split_share, number_splits):\n",
    "        splits = [] # each entry has format: [(temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y)]\n",
    "        # todo repeat the following number_splits times and add each time to splits\n",
    "        for i in range(number_splits):\n",
    "            temp_split_indices = random.sample(range(len(train_x)), int(split_share * len(train_x)))\n",
    "            temp_split_train_x = [train_x[i] for i in temp_split_indices]\n",
    "            temp_split_test_x = [train_x[i] for i in range(len(train_x)) if i not in temp_split_indices]\n",
    "\n",
    "            temp_split_train_y = [train_y[i] for i in temp_split_indices]\n",
    "            temp_split_test_y = [train_y[i] for i in range(len(train_y)) if i not in temp_split_indices]\n",
    "            splits.append((temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y))\n",
    "        return splits\n",
    "    \n",
    "    \n",
    "    def get_split_of_train_data_pasting(self, train_x, train_y, split_share):\n",
    "        splits = [] # each entry has format: [(temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y)]\n",
    "        # todo repeat the following number_splits times and add each time to splits\n",
    "        list_of_indices_to_sample = list(range(len(train_x)))\n",
    "        while len(list_of_indices_to_sample) > 1: # need at least two \n",
    "            len_list_to_sample = len(list_of_indices_to_sample)\n",
    "            if len_list_to_sample <= int(split_share * len(train_x)) * 2:\n",
    "                temp_split_train_indices = list_of_indices_to_sample[:len_list_to_sample//2]\n",
    "                temp_split_test_indices = list_of_indices_to_sample[len_list_to_sample//2:]\n",
    "            else:    \n",
    "                temp_split_train_indices = random.sample(list_of_indices_to_sample, int(split_share * len(train_x)))\n",
    "                remaining_list_indices = [x for x in list_of_indices_to_sample if x not in temp_split_train_indices]\n",
    "                temp_split_test_indices = random.sample(remaining_list_indices,int(split_share * len(train_x)))\n",
    "                \n",
    "            # remove indices which has been sampled in temp_split_indices from list_of_indices_to_sample\n",
    "            list_of_indices_to_sample  = [x for x in list_of_indices_to_sample if x not in temp_split_train_indices + temp_split_test_indices]\n",
    "\n",
    "            temp_split_train_x = [train_x[i] for i in temp_split_train_indices]\n",
    "            temp_split_test_x = [train_x[i] for i in temp_split_test_indices]\n",
    "\n",
    "            temp_split_train_y = [train_y[i] for i in temp_split_train_indices]\n",
    "            temp_split_test_y = [train_y[i] for i in temp_split_test_indices]\n",
    "\n",
    "            splits.append((temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y))\n",
    "        return splits\n",
    "            \n",
    "        \n",
    "    def get_train_auc_of_model(self,model,splits): \n",
    "        total_auc = 0\n",
    "        for temp_split_train_x, temp_split_test_x, temp_split_train_y, temp_split_test_y in splits:\n",
    "            model.fit(temp_split_train_x, temp_split_train_y)\n",
    "            prediction_ = model.predict_proba(temp_split_test_x)\n",
    "            auc = tabular_metrics.auc_metric(temp_split_test_y, prediction_)\n",
    "            total_auc += auc\n",
    "        total_auc /= len(splits)\n",
    "        return total_auc\n",
    "        \n",
    "    def compute_weights(self, level, weight_type):\n",
    "        \n",
    "        if weight_type == f\"weighted_average\":\n",
    "            if level == 'data':\n",
    "                sum_of_auc = {}\n",
    "                for key in self.data_ens_dict.keys():\n",
    "                    model_name = key.split('-')[0]\n",
    "                    if model_name not in sum_of_auc.keys():\n",
    "                        sum_of_auc[model_name] = 0\n",
    "                    sum_of_auc[model_name] += self.data_ens_dict[key][f'{level}_auc']\n",
    "                    \n",
    "                for data_ens_key, data_ens_content in self.data_ens_dict.items():\n",
    "                    self.data_ens_dict[data_ens_key][f\"{level}_weight\"] = data_ens_content[f\"{level}_auc\"] / sum_of_auc[data_ens_key.split('-')[0]]\n",
    "                \n",
    "            else: # model here\n",
    "\n",
    "\n",
    "                #sum_of_auc = sum([i[f'{level}_auc']  for i in self.model_dict.values()])\n",
    "                \n",
    "                self.model_dict = dict(sorted(self.model_dict.items(), key=lambda item: item[1][f'{level}_auc'], reverse=True))\n",
    "                \n",
    "                sum_of_weights = 0\n",
    "                for ensemble_index, model_dict_key in enumerate(self.model_dict):\n",
    "                    if ensemble_index != len(self.model_dict) - 1:\n",
    "                        self.model_dict[model_dict_key][f\"{level}_weight\"] = (1/2)**(ensemble_index+1)\n",
    "                        sum_of_weights += self.model_dict[model_dict_key][f\"{level}_weight\"]\n",
    "                    else: \n",
    "                        self.model_dict[model_dict_key][f\"{level}_weight\"] = 1 - sum_of_weights\n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "                #for ensemble_index, model_name in self.model_dict.items():\n",
    "                #    self.model_dict[ensemble_index][f\"{level}_weight\"] = self.model_dict[ensemble_index][f\"{level}_auc\"] / sum_of_auc\n",
    "                \n",
    "          \n",
    "        elif weight_type == f\"best_performer\":\n",
    "            \n",
    "            if level == 'data':\n",
    "                max_aucs = {}\n",
    "                for key in self.data_ens_dict.keys():\n",
    "                    model_name = key.split('-')[0]\n",
    "                    if model_name not in max_aucs.keys():\n",
    "                        max_aucs[model_name] = self.data_ens_dict[key][f'{level}_auc']\n",
    "                    max_aucs[model_name] = max(max_aucs[model_name], self.data_ens_dict[key][f'{level}_auc'])\n",
    "                \n",
    "                max_auc_keys = {}\n",
    "                for data_ens_key, data_ens_content in self.data_ens_dict.items():\n",
    "                    if data_ens_content[f'{level}_auc'] == max_aucs[data_ens_key.split('-')[0]]:\n",
    "                        max_auc_keys[data_ens_key.split('-')[0]] = data_ens_key\n",
    "                \n",
    "                for data_ens_key in self.data_ens_dict.keys():\n",
    "                    if data_ens_key in max_auc_keys.values():\n",
    "                        self.data_ens_dict[data_ens_key][f'{level}_weight'] = 1\n",
    "                    else:\n",
    "                        self.data_ens_dict[data_ens_key][f'{level}_weight'] = 0\n",
    "                \n",
    "            else:\n",
    "                max_model_auc = max([i[f'{level}_auc'] for i in self.model_dict.values()])\n",
    "                \n",
    "                max_auc_key = None\n",
    "                for model_key, model_content in self.model_dict.items():\n",
    "                    if model_content[f'{level}_auc'] == max_model_auc:\n",
    "                        max_auc_key = model_key\n",
    "                        break\n",
    "                \n",
    "                for model_key in self.model_dict.keys():\n",
    "                    if model_key == max_auc_key:\n",
    "                        self.model_dict[model_key][f'{level}_weight'] = 1\n",
    "                    else:\n",
    "                        self.model_dict[model_key][f'{level}_weight'] = 0\n",
    "\n",
    "        \n",
    "    def get_splits_of_train_data(self, full_train_x, full_train_y, split_type, split_share, number_splits):\n",
    "        # compute the train data splits for later AUC computation\n",
    "        if split_type == \"simple\":\n",
    "            return self.get_split_of_train_data_simple(train_x=full_train_x, train_y=full_train_y, split_share=split_share)\n",
    "        elif split_type == \"bagging\":\n",
    "            return self.get_split_of_train_data_bagging(train_x=full_train_x, train_y=full_train_y, split_share=split_share,number_splits=number_splits)\n",
    "        elif split_type == \"pasting\": \n",
    "            return self.get_split_of_train_data_pasting(train_x=full_train_x, train_y=full_train_y, split_share=split_share)\n",
    "        else:\n",
    "            print(\"todo raise error\")\n",
    "        \n",
    "        \n",
    "    def predict_proba(self, \n",
    "                      test_xs, \n",
    "                      data_ensemble_config = None,\n",
    "                      model_ensemble_config = None,\n",
    "                      data_preprocess_config = None, \n",
    "                      multiple_models=False,\n",
    "                      pre_processing=False ):\n",
    "        \n",
    "         # ------------------- only preprocessing with one model --------------------------\n",
    "        if multiple_models is False and pre_processing is True:\n",
    "            print(\"predict_proba use only the baseline model and data-ensembling as no moe config is given! \")\n",
    "            \n",
    "            baseline_model, baseline_name = None, None\n",
    "            \n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                if \"baseline\" in model_name:\n",
    "                    baseline_model = model_content[\"model\"]\n",
    "                    baseline_name = model_name\n",
    "                    break\n",
    "    \n",
    "            if baseline_model is None:  \n",
    "                raise NotImplementedError(f'The baseline_model could not be found for predict proba.')\n",
    "\n",
    "            train_xs, train_ys = baseline_model.X_, baseline_model.y_\n",
    "            \n",
    "            # add identity\n",
    "            data_preprocess_config['sklearn_transformations'].append((\"identity\",None))\n",
    "            \n",
    "            # APPLY DATA PREPROCESSING / AUGMENTATION\n",
    "            for transformer_name, transformer in data_preprocess_config[\"sklearn_transformations\"]:\n",
    "                transformation_key = baseline_name + '-' + transformer_name\n",
    "                self.data_ens_dict[transformation_key] = {}\n",
    "\n",
    "                if transformer_name != 'identity':\n",
    "                    train_xs_t = transformer.fit_transform(train_xs)\n",
    "                    \n",
    "                    number_features = min(100,train_xs_t.shape[-1])\n",
    "                    \n",
    "                    train_xs_t = train_xs_t[:,:number_features]\n",
    "                    test_xs_t = transformer.transform(test_xs)[:,:number_features]\n",
    "                else:\n",
    "                    number_features = min(100,train_xs.shape[-1])\n",
    "\n",
    "                    train_xs_t = train_xs[:,:number_features]\n",
    "                    test_xs_t = test_xs[:,:number_features]\n",
    "                \n",
    "                self.data_ens_dict[transformation_key]['train_data'] = (train_xs_t, train_ys)\n",
    "                self.data_ens_dict[transformation_key]['test_data'] = test_xs_t\n",
    "                \n",
    "                splits = self.get_splits_of_train_data(full_train_x=train_xs_t,\n",
    "                                                       full_train_y=train_ys,\n",
    "                                                       split_type=data_ensemble_config[\"data_split_type\"], \n",
    "                                                       split_share=data_ensemble_config[\"data_split_share\"],\n",
    "                                                       number_splits=data_ensemble_config[\"data_number_splits\"])\n",
    "\n",
    "                # Compute AUC performance of train data\n",
    "                self.data_ens_dict[transformation_key]['data_auc'] = float(self.get_train_auc_of_model(model=baseline_model,\n",
    "                                                                                               splits=splits))\n",
    "                baseline_model.fit(train_xs_t, train_ys)\n",
    "                self.data_ens_dict[transformation_key]['test_preds'] = baseline_model.predict_proba(self.data_ens_dict[transformation_key]['test_data'])\n",
    "                \n",
    "            baseline_model.fit(train_xs, train_ys)\n",
    "            # after having collected the auc / ce /... scores of the models of the train set, we want to \n",
    "            # compute the weights for the final forward pass\n",
    "            self.compute_weights(level=\"data\",\n",
    "                                 weight_type=data_ensemble_config[\"data_weight_type\"])\n",
    "            \n",
    "            prediction_weighted = 0\n",
    "            for data_ens_key, data_ens_content in self.data_ens_dict.items():\n",
    "                prediction_weighted += data_ens_content['test_preds'].copy() * data_ens_content['data_weight']\n",
    "                \n",
    "            # Storing the weighted predictions as predictions of the model as well\n",
    "            self.model_dict[baseline_name]['test_preds'] = prediction_weighted\n",
    "            \n",
    "            return prediction_weighted\n",
    "\n",
    "            \n",
    "        # ------------------------- only model ensembling ----------------------------------------\n",
    "        elif multiple_models is True and pre_processing is False:\n",
    "            print(\"predict_proba use moe ensembling and no data-ensembling as no data preprocessing is given\")\n",
    "            \n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                \n",
    "                train_xs, train_ys = model_content['model'].X_, model_content['model'].y_\n",
    "                \n",
    "                # compute the train data splits for later AUC computation\n",
    "                splits = self.get_splits_of_train_data(full_train_x=train_xs,\n",
    "                                                       full_train_y=train_ys,\n",
    "                                                       split_type=model_ensemble_config[\"model_split_type\"], \n",
    "                                                       split_share=model_ensemble_config[\"model_split_share\"],\n",
    "                                                       number_splits=model_ensemble_config[\"model_number_splits\"])\n",
    "\n",
    "                \n",
    "                # Compute AUC performance of train data \n",
    "                self.model_dict[model_name]['model_auc'] = float(self.get_train_auc_of_model(model=model_content['model'],\n",
    "                                                                                                splits=splits))\n",
    "\n",
    "                self.model_dict[model_name]['model'].fit(train_xs, train_ys) # overwrite the training data in the model again!\n",
    "                model_content['test_preds'] = model_content['model'].predict_proba(test_xs)\n",
    "            \n",
    "            # after having collected the auc scores of the models of the train set, we want to compute the weights for the final forward pass\n",
    "            self.compute_weights(level=\"model\",\n",
    "                                 weight_type=model_ensemble_config[\"model_weight_type\"])\n",
    "           \n",
    "            prediction_weighted = 0\n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                prediction_weighted += model_content['test_preds'].copy() * model_content['model_weight']\n",
    "                          \n",
    "            return prediction_weighted\n",
    "        \n",
    "        \n",
    "        # ---------------------- both datapreprocessing for each in moe ensembling ------------------------------    \n",
    "        else: \n",
    "            print(\"data preprocessing and moe both used!\" )\n",
    "            \n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                    \n",
    "                train_xs, train_ys = model_content['model'].X_, model_content['model'].y_\n",
    "\n",
    "                # add identity\n",
    "                data_preprocess_config['sklearn_transformations'].append((\"identity\",None))\n",
    "\n",
    "                # APPLY DATA PREPROCESSING / AUGMENTATION\n",
    "                for transformer_name, transformer in data_preprocess_config[\"sklearn_transformations\"]:\n",
    "                    transformation_key = model_name + '-' + transformer_name\n",
    "                    self.data_ens_dict[transformation_key] = {}\n",
    "\n",
    "                    if transformer_name != 'identity':\n",
    "                        train_xs_t = transformer.fit_transform(train_xs)\n",
    "\n",
    "                        number_features = min(100,train_xs_t.shape[-1])\n",
    "\n",
    "                        train_xs_t = train_xs_t[:,:number_features]\n",
    "                        test_xs_t = transformer.transform(test_xs)[:,:number_features]\n",
    "                    else:\n",
    "                        number_features = min(100,train_xs.shape[-1])\n",
    "\n",
    "                        train_xs_t = train_xs[:,:number_features]\n",
    "                        test_xs_t = test_xs[:,:number_features]\n",
    "\n",
    "\n",
    "                    self.data_ens_dict[transformation_key]['train_data'] = (train_xs_t, train_ys)\n",
    "                    self.data_ens_dict[transformation_key]['test_data'] = test_xs_t\n",
    "\n",
    "                    splits = self.get_splits_of_train_data(full_train_x=train_xs_t,\n",
    "                                                           full_train_y=train_ys,\n",
    "                                                           split_type=data_ensemble_config[\"data_split_type\"], \n",
    "                                                           split_share=data_ensemble_config[\"data_split_share\"],\n",
    "                                                           number_splits=data_ensemble_config[\"data_number_splits\"])\n",
    "\n",
    "\n",
    "                    # Compute AUC performance of train data\n",
    "                    self.data_ens_dict[transformation_key]['data_auc'] = float(self.get_train_auc_of_model(model=model_content['model'],\n",
    "                                                                                                           splits=splits))\n",
    "                    model_content['model'].fit(train_xs_t, train_ys)\n",
    "                    \n",
    "                    self.data_ens_dict[transformation_key]['test_preds'] = model_content['model'].predict_proba(self.data_ens_dict[transformation_key]['test_data'])\n",
    "               \n",
    "                    \n",
    "                    \n",
    "\n",
    "                model_content['model'].fit(train_xs, train_ys)\n",
    "                # after having collected the auc scores of the models of the train set, we want to \n",
    "            \n",
    "            # compute the weights for the data ensembles\n",
    "            self.compute_weights(level=\"data\", weight_type=data_ensemble_config[\"data_weight_type\"])\n",
    "            \n",
    "            \n",
    "            for model_name in self.model_dict.keys():\n",
    "                model_prediction_weighted = 0\n",
    "                model_auc_weighted = 0\n",
    "                for data_ens_name, data_ens_content in self.data_ens_dict.items():\n",
    "                    if data_ens_name.split('-')[0] == model_name:\n",
    "                        #print(data_ens_content)\n",
    "                        model_prediction_weighted += data_ens_content['test_preds'].copy() * \\\n",
    "                                                                                    data_ens_content['data_weight']\n",
    "                        model_auc_weighted += data_ens_content['data_auc'] * \\\n",
    "                                                                                    data_ens_content['data_weight']\n",
    "                self.model_dict[model_name]['test_preds'] = model_prediction_weighted\n",
    "                self.model_dict[model_name]['model_auc'] = model_auc_weighted\n",
    "            \n",
    "            # compute the weights for the final model ensemble\n",
    "            self.compute_weights(level=\"model\",\n",
    "                                 weight_type=model_ensemble_config[\"model_weight_type\"])\n",
    "            \n",
    "            prediction_weighted = 0\n",
    "            for model_name, model_content in self.model_dict.items():\n",
    "                prediction_weighted += model_content['test_preds'] * model_content['model_weight']\n",
    "\n",
    "            return prediction_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36af28e",
   "metadata": {},
   "source": [
    "# Computation of ensembling with a diverse set of trained TabPFNs specialized for different priors\n",
    "## Function Description\n",
    "\n",
    "In the following cell, we conduct an evaluation of ensembling using a multitude of differently trained TabPFNs, each acting as an \"expert\" specialized for a specific prior distribution. The primary goal is to explore the effectiveness of ensembling these experts on test data, leveraging their expertise in handling distinct underlying structures.\n",
    "\n",
    "### Weighting Metric:\n",
    "For this evaluation, we employ the \"model_auc\" as the weighting metric. This metric allows us to assess the performance of the ensembled models against the baseline tabpfn predictions, where the input data and corresponding labels are known during the training phase.\n",
    "\n",
    "### Weight Types and Split Types:\n",
    "To evaluate the ensembling, we utilize different weight types and split types in combination with the weighting metric. These configurations help predict the performance of the tabpfn when combined with a specific strategy to weigh the various tabpfn instances to improve the performance of the test data, based on the performance on the training data.\n",
    "\n",
    "### Specialized Prior Distributions:\n",
    "The evaluated TabPFNs have been trained using three distinct types of prior distributions:\n",
    "1. **Gaussian Process Priors (GPs):** Some experts have been trained exclusively on Gaussian process priors, making them proficient in handling datasets governed by such distributions.\n",
    "2. **Structural Causal Models (SCMs):** Other experts have been trained solely on structural causal models, allowing them to excel in datasets governed by these specific structures.\n",
    "3. **Mixture of GPs and SCMs:** Several experts have been trained on a combination of Gaussian process priors and structural causal models, enabling them to handle datasets exhibiting mixed underlying structures.\n",
    "\n",
    "### Aggregation and Weighting Strategies:\n",
    "Following the forward pass with the loaded data, we adopt diverse strategies to aggregate and weight the results of the TabPFNs based on their performance on the training data's AUC. These strategies aim to leverage the strengths of each expert, resulting in a combined model that performs better in their respective \"expertise\" fields.\n",
    "\n",
    "### Main Idea:\n",
    "The main idea behind this evaluation is to assemble a diverse set of experts with specialized knowledge in handling different types of underlying structures. By ensembling these experts, we seek to improve the overall performance of the combined model compared to the baseline tabpfn. The evaluation proceeds through multiple steps, each comprising a unique configuration of weighting metric, weight types, and split types in conjunction with various tabpfn instances.\n",
    "\n",
    "The computed results for each configuration are stored in the \"ensemble_results\" dictionary, enabling us to analyze the impact of each ensemble approach on the model's performance. This evaluation plays a crucial role in understanding how ensembling can harness the expertise of diverse specialists to achieve enhanced performance across a wide range of datasets. As a result, it provides valuable insights into leveraging specialized knowledge to tackle complex real-world problems effectively.\n",
    "\n",
    "The evaluation cell first checks for previously computed results and loads them from the file if available. If not, it performs the evaluation for each dataset within the specified range of \"number_iterations.\" For each dataset, we compare the performance of the tabpfn model with the baseline model and compute their respective AUC metrics. We then proceed to perform ensembling for various configurations, calculating the AUC metrics for each ensemble and storing the results.\n",
    "\n",
    "Finally, the evaluation results are saved to a pickle file for future reference and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5c65f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:42:33.158226Z",
     "start_time": "2023-07-22T09:41:40.794306Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluations_folder = \"./evaluations/experts/\"\n",
    "file_name = f\"{evaluations_folder}only_experts_ensemble_results.pickle\"\n",
    "\n",
    "\n",
    "model_paths = [\"./models_gp\",\"./models_causal\", \"./models_bag\"]#,\"./baseline\"]    \n",
    "    \n",
    "baseline_classifier = TabPFNClassifier(device=device, \n",
    "                                     base_path=\"./baseline\",\n",
    "                                     model_string=\"model_bag_baseline\",\n",
    "                                     N_ensemble_configurations=1,\n",
    "                                     batch_size_inference=1, \n",
    "                                     no_preprocess_mode=True)\n",
    "\n",
    "# Ensemble configuration \n",
    "\n",
    "model_weighting_metrics = [\"model_auc\"]\n",
    "model_weight_types = [\"best_performer\",\"weighted_average\"] \n",
    "model_split_types =[\"simple\", \"bagging\", \"pasting\"]\n",
    "\n",
    "\n",
    "# Configure run \n",
    "ensemble_results = {}\n",
    "\n",
    "# Indices of the datasets to predict\n",
    "start = 0\n",
    "end =  2\n",
    "number_iterations = range(max(start, 0),min(end,len(test_datasets)))\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        ensemble_results = pickle.load(file)\n",
    "else:\n",
    "    for dataset_index in number_iterations:\n",
    "\n",
    "        train_xs, train_ys, test_xs, test_ys, dataset_name = prepare_dataset_for_evaluation(dataset_index=dataset_index, test_datasets=test_datasets)\n",
    "\n",
    "        # compare to normal tabpfn\n",
    "        baseline_classifier.fit(train_xs, train_ys)\n",
    "        prediction_tabpfn = baseline_classifier.predict_proba(test_xs)\n",
    "        auc_baseline = tabular_metrics.auc_metric(test_ys, prediction_tabpfn)\n",
    "\n",
    "        # add computed results to ensemble_results\n",
    "        if \"baseline\" not in ensemble_results:\n",
    "            ensemble_results[\"baseline\"] = {}\n",
    "\n",
    "        if dataset_name not in ensemble_results[\"baseline\"]:\n",
    "            ensemble_results[\"baseline\"][dataset_name] = {}\n",
    "\n",
    "\n",
    "        baseline_dict = {\"auc\":auc_baseline.item()}\n",
    "        ensemble_results[\"baseline\"][dataset_name]= baseline_dict\n",
    "\n",
    "\n",
    "        for weighting_metric in model_weighting_metrics: \n",
    "            for weight_type in model_weight_types: \n",
    "                for split_type in model_split_types:\n",
    "                    print(f\"Current configuration: weighting_metric {weighting_metric}, weight_type {weight_type}, split_type {split_type}\")\n",
    "\n",
    "\n",
    "                    model_ensemble_config = {\"model_weighting_metric\":weighting_metric,\n",
    "                                             \"model_weight_type\":weight_type,\n",
    "                                             \"model_split_type\":split_type,\n",
    "                                             \"model_split_share\":0.5, \n",
    "                                             \"model_number_splits\": 5}\n",
    "\n",
    "                    data_preprocess_config = None\n",
    "                    data_ensemble_config = None\n",
    "\n",
    "                    # create classifier with configuration \n",
    "                    classifier_ensemble = PFNEnsemble(model_storage_folders=model_paths, N_ensemble_configurations=1, device=device)\n",
    "\n",
    "                    classifier_ensemble.fit(train_xs, train_ys)\n",
    "                    prediction_ = classifier_ensemble.predict_proba(test_xs,\n",
    "                                                                    data_ensemble_config=data_ensemble_config,\n",
    "                                                                    model_ensemble_config=model_ensemble_config,\n",
    "                                                                    data_preprocess_config=data_preprocess_config, \n",
    "                                                                    multiple_models=True,\n",
    "                                                                    pre_processing=False)\n",
    "\n",
    "                    auc_ensemble = tabular_metrics.auc_metric(test_ys, prediction_)\n",
    "                    \n",
    "                    # add computed results to results dict\n",
    "                    ensemble_name = f\"{weight_type}-{split_type}\"\n",
    "\n",
    "                    if ensemble_name not in ensemble_results:\n",
    "                        ensemble_results[ensemble_name] = {}\n",
    "\n",
    "                    if dataset_name not in ensemble_results[ensemble_name]:\n",
    "                        ensemble_results[ensemble_name][dataset_name] = {}\n",
    "                        \n",
    "                    ensemble_dict = {\"auc\":auc_ensemble.item(),\n",
    "                                    \"model_dict\":classifier_ensemble.model_dict,\n",
    "                                    \"data_ens_dict\":classifier_ensemble.data_ens_dict}\n",
    "\n",
    "                    ensemble_results[ensemble_name][dataset_name]= ensemble_dict\n",
    "                    \n",
    "    save_dict_to_pickle_file(ensemble_results, file_name, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c01ef7",
   "metadata": {},
   "source": [
    "## ONLY EXPERT EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d770165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:40:01.938414Z",
     "start_time": "2023-07-22T09:40:01.910265Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "##---------------------------- S T A R T - E V A L U A T I O N - P L O T S ----------------------------\n",
    "ensemble_results = clean_result_dictionary_for_evaluation(ensemble_results, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])\n",
    "\n",
    "##---------------------------- H E A T M A P S ----------------------------\n",
    "start, stop = 0, 15\n",
    "\n",
    "dataset_names = [dataset[0] for dataset_index,dataset in enumerate(test_datasets) if start <= dataset_index <= stop]\n",
    "\n",
    "dataset_names= [\"analcatdata_dmft\",\"cmc\", \"credit-approval\", \"credit-g\", \"diabetes\", \"eucalyptus\",\n",
    "                \"mfeat-fourier\", \"mfeat-karhunen\", \"mfeat-morphological\", \"mfeat-zernike\", \"tic-tac-toe\", \n",
    "                \"blood-transfusion-service-center\", \"climate-model-simulation-crashes\", \"cylinder-bands\", \n",
    "                \"steel-plates-fault\" ]\n",
    "\n",
    "plot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, f\"{evaluations_folder}dataset[selection]-plot_heatmap_auc_datasets_configs.png\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##------------------------------------------------------------------------------------------------------------\n",
    "##---------------------------- T I M E S - M O D E L - H I S T O G R A M  ------------------------------------\n",
    "\n",
    "plot_histogram_times_picked_model(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_model.png\" )\n",
    "\n",
    "##---------------------------- S T A T I S T I C A L - T E S T S -------------------------\n",
    "ttest_auc_performance_compared_to_baseline(dictionary=ensemble_results, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c09a4f",
   "metadata": {},
   "source": [
    "# Computation of all ensemble configurations with only data-ensembling\n",
    "## Function Description\n",
    "\n",
    "In the following cell, we perform an evaluation of ensembling using various data transformations in combination with the baseline tabpfn instance. The purpose of this evaluation is to investigate how different preprocessing techniques impact the performance of the tabpfn model combined with different techniques on ensembling on test data compared to the original baseline results.\n",
    "\n",
    "### Weighting Metric:\n",
    "The evaluation uses \"data_auc\" as the weighting metric. This metric is employed to assess the performance of the ensembled models against the baseline tabpfn predictions based on the auc of the training data where the input as well as the labels are known to the model.\n",
    "\n",
    "### Weight Types and Split Types:\n",
    "To evaluate the ensembling, different weight types and split types are utilized in combination with the weighting metric. These configurations are used to predict the performance of the tabpfn when combined with a specific data transformation of the test data based on the train data.\n",
    "\n",
    "### Data Preprocessing Functionalities:\n",
    "The data preprocessing functionalities involve two main steps:\n",
    "1. **Bagging with Different Feature Permutations:** This process entails applying bagging with various feature permutations to the data. Bagging is a technique that involves training multiple instances of the same model with different subsets of the training data and then combining their predictions. In this case, feature permutations refer to rearranging the order of features to create different subsets for training as the tabpfn transformer is not invariant to the feature order.\n",
    "\n",
    "2. **Different Data Transformations from sklearn Library:** Several data transformations from the sklearn library are utilized for preprocessing. These transformations include:\n",
    "   - PCA (Principal Component Analysis)\n",
    "   - KernelPCA (Kernel Principal Component Analysis)\n",
    "   - TruncatedSVD (Truncated Singular Value Decomposition)\n",
    "   - FastICA (Fast Independent Component Analysis)\n",
    "   - FeatureAgglomeration (Feature Agglomeration)\n",
    "   - RobustScaler (Robust Scaler)\n",
    "   - PowerTransformer (Power Transformer)\n",
    "   - QuantileTransformer (Quantile Transformer)\n",
    "\n",
    "### Main Idea:\n",
    "The main idea behind this evaluation is to determine whether a specific data preprocessing technique allows the tabpfn model to extract the underlying distribution's structure more effectively. By comparing the ensembled model's performance with different preprocessing configurations to the baseline tabpfn model, it becomes possible to identify which data transformation improves the tabpfn's performance.\n",
    "\n",
    "The evaluation proceeds in multiple steps, with each step comprising a distinct configuration of weighting metric, weight types, and split types in conjunction with different data transformations. The computed results for each configuration are stored in the \"ensemble_results\" dictionary.\n",
    "\n",
    "The evaluation cell begins by loading previously computed results from the file if available. Otherwise, it performs the evaluation for each dataset within the specified range of \"number_iterations.\" For each dataset, the tabpfn is compared to the baseline model, and their respective AUC (Area Under the Curve) metrics are computed. Then, the ensembling is performed for various configurations, and the AUC metrics for each ensemble are computed and stored.\n",
    "\n",
    "Finally, the results are saved to a pickle file for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa69c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:40:02.234453Z",
     "start_time": "2023-07-22T09:40:01.941414Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# C O N F I G U R A T I O N \n",
    "model_paths = [\"./baseline\"]\n",
    "\n",
    "evaluations_folder = \"./evaluations/data/\"\n",
    "\n",
    "file_name = f\"{evaluations_folder}only_data_ensemble_results.pickle\"\n",
    "\n",
    "baseline_classifier = TabPFNClassifier(device=device, base_path=\"./baseline\", model_string=\"model_bag_baseline\",\n",
    "                                     N_ensemble_configurations=1, batch_size_inference=1, no_preprocess_mode=True,\n",
    "                                     multiclass_decoder='permutation', feature_shift_decoder=False)\n",
    "\n",
    "# Ensemble configuration \n",
    "weighting_metrics = [\"data_auc\"]\n",
    "weight_types = [\"best_performer\",\"weighted_average\"]\n",
    "split_types =[\"simple\", \"bagging\", \"pasting\"]\n",
    "\n",
    "# Data preprocessing configuration\n",
    "data_preprocess_config = {\"N_ensemble_configurations\": 1,\n",
    "                          \"sklearn_transformations\": [\n",
    "                              (\"RobustScaler_no_unit_variance\",RobustScaler(unit_variance=False)),\n",
    "                              (\"RobustScaler_no_unit_variance\",RobustScaler(unit_variance=True)),\n",
    "                              (\"PowerTransformer\",PowerTransformer()),\n",
    "                              (\"PowerTransformer\",PowerTransformer(standardize=False)),\n",
    "                              (\"MaxAbsScaler\",MaxAbsScaler()),\n",
    "                              (\"QuantileTransformer_100\",QuantileTransformer(n_quantiles=100)),\n",
    "                              (\"QuantileTransformer_250\",QuantileTransformer(n_quantiles=250)),\n",
    "                              (\"QuantileTransformer_50\",QuantileTransformer(n_quantiles=50))\n",
    "                                                  ]\n",
    "                         }\n",
    "\n",
    "# Configure run \n",
    "ensemble_results = {}\n",
    "\n",
    "# Indices of the datasets to predict\n",
    "start = 0\n",
    "end =  30\n",
    "number_iterations = range(max(start, 0),min(end,len(test_datasets)))\n",
    "\n",
    "    \n",
    "#---------------------------- S T A R T - E V A L U A T I O N - R U N ----------------------------\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        ensemble_results = pickle.load(file)\n",
    "else:\n",
    "    for dataset_index in number_iterations:\n",
    "        train_xs, train_ys, test_xs, test_ys, dataset_name = prepare_dataset_for_evaluation(dataset_index=dataset_index, test_datasets=test_datasets)\n",
    "        \n",
    "        \n",
    "        # compare to baseline tabpfn\n",
    "        baseline_classifier.fit(train_xs, train_ys)\n",
    "        prediction_tabpfn = baseline_classifier.predict_proba(test_xs)\n",
    "        auc_baseline = tabular_metrics.auc_metric(test_ys, prediction_tabpfn)\n",
    "\n",
    "        # add computed results to ensemble_results\n",
    "        if \"baseline\" not in ensemble_results:\n",
    "            ensemble_results[\"baseline\"] = {}\n",
    "\n",
    "        if dataset_name not in ensemble_results[\"baseline\"]:\n",
    "            ensemble_results[\"baseline\"][dataset_name] = {}\n",
    "\n",
    "\n",
    "        baseline_dict = {\"auc\":auc_baseline.item()}\n",
    "        ensemble_results[\"baseline\"][dataset_name]= baseline_dict\n",
    "\n",
    "        # Ensemble Configurations \n",
    "        for weighting_metric in weighting_metrics: \n",
    "            for weight_type in weight_types: \n",
    "                for split_type in split_types:\n",
    "                    print(f\"Current configuration: weighting_metric {weighting_metric}, weight_type {weight_type}, split_type {split_type}\")\n",
    "\n",
    "                    data_ensemble_config = {\"data_weighting_metric\":weighting_metric,\n",
    "                                            \"data_weight_type\":weight_type,\n",
    "                                            \"data_split_type\":split_type,\n",
    "                                            \"data_split_share\":0.5, \n",
    "                                            \"data_number_splits\": 3}\n",
    "\n",
    "                    \n",
    "                    # create classifier with configuration \n",
    "                    classifier_ensemble = PFNEnsemble(model_storage_folders=model_paths,\n",
    "                                                      N_ensemble_configurations=data_preprocess_config[\"N_ensemble_configurations\"],\n",
    "                                                      device=device)\n",
    "\n",
    "                    classifier_ensemble.fit(train_xs, train_ys)\n",
    "                    prediction_ = classifier_ensemble.predict_proba(test_xs,\n",
    "                                                                    model_ensemble_config= None,\n",
    "                                                                    data_ensemble_config=data_ensemble_config,\n",
    "                                                                    data_preprocess_config=data_preprocess_config,\n",
    "                                                                    multiple_models=False,\n",
    "                                                                    pre_processing=True )\n",
    "\n",
    "                    auc_ensemble  = tabular_metrics.auc_metric(test_ys, prediction_)\n",
    "\n",
    "                    # add computed results to results dict\n",
    "                    ensemble_name = f\"{weight_type}-{split_type}\"\n",
    "\n",
    "                    if ensemble_name not in ensemble_results:\n",
    "                        ensemble_results[ensemble_name] = {}\n",
    "\n",
    "                    if dataset_name not in ensemble_results[ensemble_name]:\n",
    "                        ensemble_results[ensemble_name][dataset_name] = {}\n",
    "                        \n",
    "                    ensemble_dict = {\"auc\":auc_ensemble.item(),\n",
    "                                    \"model_dict\":classifier_ensemble.model_dict,\n",
    "                                    \"data_ens_dict\":classifier_ensemble.data_ens_dict}\n",
    "\n",
    "                    ensemble_results[ensemble_name][dataset_name]= ensemble_dict\n",
    "                    \n",
    "    save_dict_to_pickle_file(ensemble_results, file_name, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919cea89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:40:02.250657Z",
     "start_time": "2023-07-22T09:40:02.237983Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "##---------------------------- S T A R T - E V A L U A T I O N - P L O T S ----------------------------\n",
    "#\n",
    "ensemble_results = clean_result_dictionary_for_evaluation(ensemble_results, keys_to_exclude=[\"model\", \"train_data\", \"test_data\", \"test_preds\"])\n",
    "#print(ensemble_results)\n",
    "##---------------------------- H E A T M A P S ----------------------------\n",
    "# FOR THE FINAL EVALUATION\n",
    "\n",
    "dataset_names = [ 'mfeat-fourier',\n",
    " 'mfeat-morphological',\n",
    " 'credit-approval',\n",
    " 'diabetes', \n",
    " 'tic-tac-toe',\n",
    " 'vehicle', \n",
    " 'analcatdata_dmft',\n",
    " 'pc4','pc4', 'pc3',\n",
    " 'kc2', 'pc1', 'ilpd', 'climate-model-simulation-crashes']\n",
    "plot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, f\"{evaluations_folder}data_dataset[selection]-plot_heatmap_auc_datasets_configs.png\" )\n",
    "\n",
    "#\n",
    "##------------------------------------------------------------------------------------------------------------\n",
    "##---------------------------- I N S T A N C E S - A U C - H I S T O G R A M  --------------------------------\n",
    "\n",
    "# FOR THE FINAL EVALUATION\n",
    "plot_histogram_average_train_AUC_instance(ensemble_results, f\"{evaluations_folder}plot_histogram_average_test_AUC_instance.png\")\n",
    "\n",
    "##---------------------------- S T A T I S T I C A L - T E S T S -------------------------\n",
    "ttest_auc_performance_compared_to_baseline(dictionary=ensemble_results,alpha=0.05)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f956792",
   "metadata": {},
   "source": [
    "# Combined Data and Expert Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c9a45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T10:39:42.070168Z",
     "start_time": "2023-07-22T10:38:53.080317Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "evaluations_folder = \"./evaluations/combined/\"\n",
    "file_name = f\"{evaluations_folder}combined_data_and_experts_ensemble_results.pickle\"\n",
    "\n",
    "\n",
    "model_paths = [\"./models_gp\",\"./models_causal\",\"./models_bag\"]    \n",
    "\n",
    "baseline_classifier = TabPFNClassifier(device=device, \n",
    "                                     base_path=\"./baseline\",\n",
    "                                     model_string=\"model_bag_baseline\",\n",
    "                                     N_ensemble_configurations=1,\n",
    "                                     batch_size_inference=1, \n",
    "                                     no_preprocess_mode=True,\n",
    "                                     multiclass_decoder='permutation',\n",
    "                                     feature_shift_decoder=False)#??? \n",
    "\n",
    "data_weighting_metrics = [\"data_auc\"]  \n",
    "data_weight_types = [\"best_performer\"]#,\"weighted_average\"] \n",
    "data_split_types =[\"simple\"]#, \"bagging\", \"pasting\"] \n",
    "\n",
    "model_weighting_metrics = [\"model_auc\"]  \n",
    "model_weight_types = [\"best_performer\"]#,\"weighted_average\"] \n",
    "model_split_types =[\"simple\"]#, \"bagging\", \"pasting\"] \n",
    "\n",
    "data_preprocess_config = {\"N_ensemble_configurations\": 1,\n",
    "                          \"sklearn_transformations\": [(\"PCA\",PCA()),\n",
    "                                                      (\"KernelPCA\",KernelPCA()),\n",
    "                                                      (\"TruncatedSVD\",TruncatedSVD()),\n",
    "                                                      (\"FastICA\",FastICA()),\n",
    "                                                      (\"FeatureAgglomeration\",FeatureAgglomeration()),\n",
    "                                                      (\"RobustScaler\",RobustScaler()),\n",
    "                                                      (\"PowerTransformer\",PowerTransformer()),\n",
    "                                                      (\"QuantileTransformer\",QuantileTransformer(n_quantiles=100))\n",
    "                                                     ]\n",
    "                         }\n",
    "\n",
    "# Configure run \n",
    "ensemble_results = {}\n",
    "\n",
    "start = 0\n",
    "end = 5\n",
    "number_iterations = range(max(start, 0),min(end,len(test_datasets)))# Index of the dataset to predict\n",
    "\n",
    "if os.path.exists(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        ensemble_results = pickle.load(file)\n",
    "else:\n",
    "    for evaluation_dataset_index in number_iterations:\n",
    "        train_xs, train_ys, test_xs, test_ys, dataset_name = prepare_dataset_for_evaluation(dataset_index=evaluation_dataset_index, test_datasets=test_datasets)\n",
    "        \n",
    "        # compare to baseline tabpfn\n",
    "        baseline_classifier.fit(train_xs, train_ys)\n",
    "        prediction_tabpfn = baseline_classifier.predict_proba(test_xs)\n",
    "        auc_baseline = tabular_metrics.auc_metric(test_ys, prediction_tabpfn)\n",
    "\n",
    "        # add computed results to ensemble_results\n",
    "        if \"baseline\" not in ensemble_results:\n",
    "            ensemble_results[\"baseline\"] = {}\n",
    "\n",
    "        if dataset_name not in ensemble_results[\"baseline\"]:\n",
    "            ensemble_results[\"baseline\"][dataset_name] = {}\n",
    "\n",
    "\n",
    "        baseline_dict = {\"auc\":auc_baseline.item()}\n",
    "        ensemble_results[\"baseline\"][dataset_name]= baseline_dict\n",
    "        \n",
    "        \n",
    "        # MoE Configurations ---------------------\n",
    "        for model_weighting_metric in model_weighting_metrics: \n",
    "            for model_weight_type in model_weight_types: \n",
    "                for model_split_type in model_split_types:\n",
    "                    print(f\"Current model_configuration: model_weighting_metric {model_weighting_metric}, model_weight_type {model_weight_type}, model_split_type {model_split_type}\")\n",
    "                    # --------------------------------\n",
    "                    model_ensemble_config = {\"model_weighting_metric\":model_weighting_metric,\n",
    "                                             \"model_weight_type\":model_weight_type,\n",
    "                                             \"model_split_type\":model_split_type,\n",
    "                                             \"model_split_share\":0.5, \n",
    "                                             \"model_number_splits\": 5}\n",
    "\n",
    "             \n",
    "                    # create classifier with configuration \n",
    "                    classifier_ensemble = PFNEnsemble(model_storage_folders=model_paths,\n",
    "                                               N_ensemble_configurations=data_preprocess_config[\"N_ensemble_configurations\"],\n",
    "                                               device=device)\n",
    "\n",
    "                    classifier_ensemble.fit(train_xs, train_ys)\n",
    "                    \n",
    "                    for data_weighting_metric in data_weighting_metrics: \n",
    "                            for data_weight_type in data_weight_types: \n",
    "                                for data_split_type in data_split_types:\n",
    "\n",
    "                                    data_ensemble_config = {\"data_weighting_metric\":data_weighting_metric,\n",
    "                                                            \"data_weight_type\":data_weight_type,\n",
    "                                                            \"data_split_type\":data_split_type,\n",
    "                                                            \"data_split_share\":0.5, \n",
    "                                                            \"data_number_splits\": 5}\n",
    "\n",
    "                                    start_time = time.time()\n",
    "\n",
    "                                    prediction_ = classifier_ensemble.predict_proba(test_xs,\n",
    "                                                                                    model_ensemble_config=model_ensemble_config,\n",
    "                                                                                    data_ensemble_config=data_ensemble_config,\n",
    "                                                                                    data_preprocess_config=data_preprocess_config,\n",
    "                                                                                    multiple_models=True,\n",
    "                                                                                    pre_processing=True)\n",
    "\n",
    "\n",
    "                                    # Print the evaluation time\n",
    "                                    print(f\"Evaluation predict proba time: { time.time() - start_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "                                    auc_ensemble = tabular_metrics.auc_metric(test_ys, prediction_)\n",
    "                                    \n",
    "                                    # add computed results to results dict\n",
    "                                    ensemble_name = f\"m_{model_weight_type}-m_{model_split_type}-d_{data_weight_type}-d_{data_split_type}\"\n",
    "\n",
    "                                    if ensemble_name not in ensemble_results:\n",
    "                                        ensemble_results[ensemble_name] = {}\n",
    "\n",
    "                                    if dataset_name not in ensemble_results[ensemble_name]:\n",
    "                                        ensemble_results[ensemble_name][dataset_name] = {}\n",
    "\n",
    "                                    ensemble_dict = {\"auc\":auc_ensemble.item(),\n",
    "                                                    \"model_dict\":classifier_ensemble.model_dict,\n",
    "                                                    \"data_ens_dict\":classifier_ensemble.data_ens_dict}\n",
    "\n",
    "                                    ensemble_results[ensemble_name][dataset_name]= ensemble_dict\n",
    "\n",
    "    save_dict_to_pickle_file(ensemble_results, file_name, keys_to_exclude= [\"model\", \"train_data\", \"test_data\", \"test_preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492a6910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-22T09:40:02.472725Z",
     "start_time": "2023-07-22T09:39:54.339Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_scatter_train_auc_test_auc_weighted_average_combined(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_weighted_average_combined.png\")\n",
    "\n",
    "'''\n",
    "\n",
    "#---------------------------- S T A R T - E V A L U A T I O N - P L O T S ----------------------------\n",
    "\n",
    "#---------------------------- H E A T M A P S ----------------------------\n",
    "start, stop = 0, 5\n",
    "\n",
    "dataset_names = [dataset[0] for dataset_index,dataset in enumerate(test_datasets) if start <= dataset_index <= stop]\n",
    "plot_heatmap_auc_datasets_configs(ensemble_results, dataset_names, f\"{evaluations_folder}dataset[{start}-{stop}]-plot_heatmap_auc_datasets_configs.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- A V E R A G E - W EI G H T - H I S T O G R A M -------------------------\n",
    "transformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \n",
    "#plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\n",
    "\n",
    "\n",
    "#---------------------------- S C A T T E R -------------------------\n",
    "#plot_scatter_train_auc_test_auc_best_performer_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_best_performer_transformations.png\")\n",
    "#plot_scatter_train_auc_test_auc_weighted_average_transformations(ensemble_results, f\"{evaluations_folder}plot_scatter_train_auc_test_auc_weighted_average_transformations.png\")\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- T R A N S F O R M A T I O N S - H I S T O G R A M  ----------------------------\n",
    "transformation_names =[ transformation[0] for transformation in data_preprocess_config[\"sklearn_transformations\"]]        \n",
    "#plot_histogram_average_weights_data_transformations(ensemble_results, transformation_names,f\"{evaluations_folder}plot_histogram_average_weights_data_transformations.png\" )\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- M O D E L - A U C - H I S T O G R A M  ----------------------------------------\n",
    "plot_histogram_average_train_auc_model(ensemble_results, f\"{evaluations_folder}plot_histogram_average_train_auc_model.png\" )\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "#---------------------------- T I M E S - M O D E L - H I S T O G R A M  ------------------------------------\n",
    "plot_histogram_times_picked_model(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_model.png\" )\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "#------------------ T I M E S - T R A N S F O R M A T I O N - H I S T O G R A M  ------------------\n",
    "#plot_histogram_times_picked_transformation(ensemble_results, f\"{evaluations_folder}plot_histogram_times_picked_transformation.png\" )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------- S T A T I S T I C A L - T E S T S -------------------------\n",
    "ttest_auc_performance_compared_to_baseline(dictionary=ensemble_results,alpha=0.05)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ba40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "tabpfn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
